sparse_data_train_svm <- sparse_data_train[, selected_indices_svm, drop = F]
library(data.table)
library(dplyr)
library(ggplot2)
library(caret)
library(ISLR)
data <- read.csv("data2.csv.gz")
# Iterate through label and make them 0 if they are -1, 1 otherwise
for (i in 1:nrow(data)) {
if (data[i, 1] == -1) {
data[i, 1] <- 0
} else {
data[i, 1] <- 1
}
}
# Find duplicate columns
unique_data <- data[, !duplicated(as.list(data))]
num_repeated <- ncol(data) - ncol(unique_data)
cat("The new dataset has dimensions:", dim(unique_data), "\n")
# Calculate row indices for each split
set.seed(12)  # Set seed for reproducibility
unique_data <- unique_data[sample(nrow(unique_data)), ]
n <- nrow(unique_data)
end_training <- floor(n * 0.7)  # 70% for training
end_validation <- floor(n * 0.85)  # Next 10% for validation
# Split the data into training, validation, and testing sets
training_data <- unique_data[1:end_training, ]  # 70% of the data
validation_data <- unique_data[(end_training + 1):end_validation, ]  # 10% of the data
testing_data <- unique_data[(end_validation + 1):n, ]  # Remaining 20% of the data
# # Use 80% of the data for training
# end_training <- floor(nrow(unique_data) * 0.8)
# # Split the data into training and testing sets
# training_data <- unique_data[1:end_training, ]
# testing_data <- unique_data[(end_training + 81):nrow(unique_data), ]
print(sum(training_data[,1]))
print(sum(testing_data[,1]))
print(sum(validation_data[,1]))
library(Matrix)
library(glmnet)
#create dep. var
y_train <- training_data[,1]
y_valid <- validation_data[,1]
y_test <- testing_data[,1]
#create training,  validation and  testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")
features_valid <- validation_data[,-1]
sparse_data_valid <- as(features_valid, "sparseMatrix")
features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")
# Install necessary packages (if not already installed)
if (!require("LiblineaR")) install.packages("LiblineaR")
if (!require("Matrix")) install.packages("Matrix")
# Load libraries
library(LiblineaR)
library(Matrix)
# y_train <- as.factor(y_train)  # Convert to factor if it's classification
tryTypes=c(1:6)
tryCosts=c(1000,0.001)
bestCost=NA
bestAcc=0
bestType=NA
for(ty in tryTypes){
for(co in tryCosts){
acc=LiblineaR(data=sparse_data_train,target=y_train,type=ty,cost=co,bias=1,cross=5,verbose=FALSE)
if(acc>bestAcc){
bestCost=co
bestAcc=acc
bestType=ty
}
}
}
cat("Best model type is:",bestType,"\n")
cat("Best cost is:",bestCost,"\n")
cat("Best accuracy is:",bestAcc,"\n")
# Install necessary packages (if not already installed)
if (!require("LiblineaR")) install.packages("LiblineaR")
if (!require("Matrix")) install.packages("Matrix")
# Load libraries
library(LiblineaR)
library(Matrix)
# y_train <- as.factor(y_train)  # Convert to factor if it's classification
tryTypes=c(1:6)
tryCosts=c(1000, 100, 1, 0.1, 0.001)
bestCost=NA
bestAcc=0
bestType=NA
for(ty in tryTypes){
for(co in tryCosts){
acc=LiblineaR(data=sparse_data_train,target=y_train,type=ty,cost=co,bias=1,cross=5,verbose=FALSE)
if(acc>bestAcc){
bestCost=co
bestAcc=acc
bestType=ty
}
}
}
cat("Best model type is:",bestType,"\n")
cat("Best cost is:",bestCost,"\n")
cat("Best accuracy is:",bestAcc,"\n")
final_model_svm <- LiblineaR(data=sparse_data_train,target=y_train,type=bestType,cost=bestCost,bias=1,verbose=FALSE)
weights_svm <- final_model_svm$W
sorted_indices_svm <- order(abs(weights_svm), decreasing = TRUE)
sorted_weights_svm <- abs(weights_svm[sorted_indices_svm])
cumulative_sum_svm <- cumsum(sorted_weights_svm)
explained_svm <- 0.5 * sum(sorted_weights_svm)
selected_indices_svm <- sorted_indices_svm[cumulative_sum_svm <= explained_svm]
# Step 4: Subset the weights_svm vector based on selected indices
selected_weights_svm <- weights_svm[selected_indices_svm]
num_features_svm_red <- length(selected_weights_svm)
print(num_features_svm_red)
final_model_svm <- LiblineaR(data=sparse_data_train,target=y_train,type=bestType,cost=bestCost,bias=1,verbose=FALSE)
weights_svm <- final_model_svm$W
sorted_indices_svm <- order(abs(weights_svm), decreasing = TRUE)
sorted_weights_svm <- abs(weights_svm[sorted_indices_svm])
cumulative_sum_svm <- cumsum(sorted_weights_svm)
explained_svm <- 0.95 * sum(sorted_weights_svm)
selected_indices_svm <- sorted_indices_svm[cumulative_sum_svm <= explained_svm]
# Step 4: Subset the weights_svm vector based on selected indices
selected_weights_svm <- weights_svm[selected_indices_svm]
num_features_svm_red <- length(selected_weights_svm)
print(num_features_svm_red)
sparse_data_train_svm <- sparse_data_train[, selected_indices_svm, drop = F]
bestType = 6
final_model_svm <- LiblineaR(data=sparse_data_train,target=y_train,type=bestType,cost=bestCost,bias=1,verbose=FALSE)
weights_svm <- final_model_svm$W
sorted_indices_svm <- order(abs(weights_svm), decreasing = TRUE)
sorted_weights_svm <- abs(weights_svm[sorted_indices_svm])
cumulative_sum_svm <- cumsum(sorted_weights_svm)
explained_svm <- 0.95 * sum(sorted_weights_svm)
selected_indices_svm <- sorted_indices_svm[cumulative_sum_svm <= explained_svm]
# Step 4: Subset the weights_svm vector based on selected indices
selected_weights_svm <- weights_svm[selected_indices_svm]
num_features_svm_red <- length(selected_weights_svm)
print(num_features_svm_red)
# Install necessary packages (if not already installed)
if (!require("LiblineaR")) install.packages("LiblineaR")
if (!require("Matrix")) install.packages("Matrix")
# Load libraries
library(LiblineaR)
library(Matrix)
# y_train <- as.factor(y_train)  # Convert to factor if it's classification
tryTypes=c(1,2,3,4,6)
tryCosts=c(1000, 100, 1, 0.1, 0.001)
bestCost=NA
bestAcc=0
bestType=NA
for(ty in tryTypes){
for(co in tryCosts){
acc=LiblineaR(data=sparse_data_train,target=y_train,type=ty,cost=co,bias=1,cross=5,verbose=FALSE)
if(acc>bestAcc){
bestCost=co
bestAcc=acc
bestType=ty
}
}
}
cat("Best model type is:",bestType,"\n")
cat("Best cost is:",bestCost,"\n")
cat("Best accuracy is:",bestAcc,"\n")
bestType = 6
final_model_svm <- LiblineaR(data=sparse_data_train,target=y_train,type=bestType,cost=bestCost,bias=1,verbose=FALSE)
weights_svm <- final_model_svm$W
sorted_indices_svm <- order(abs(weights_svm), decreasing = TRUE)
sorted_weights_svm <- abs(weights_svm[sorted_indices_svm])
cumulative_sum_svm <- cumsum(sorted_weights_svm)
explained_svm <- 0.95 * sum(sorted_weights_svm)
selected_indices_svm <- sorted_indices_svm[cumulative_sum_svm <= explained_svm]
# Step 4: Subset the weights_svm vector based on selected indices
selected_weights_svm <- weights_svm[selected_indices_svm]
num_features_svm_red <- length(selected_weights_svm)
print(num_features_svm_red)
bestType = 6
final_model_svm <- LiblineaR(data=sparse_data_train,target=y_train,type=bestType,cost=bestCost,bias=1,verbose=FALSE)
weights_svm <- final_model_svm$W
sorted_indices_svm <- order(abs(weights_svm), decreasing = TRUE)
sorted_weights_svm <- abs(weights_svm[sorted_indices_svm])
cumulative_sum_svm <- cumsum(sorted_weights_svm)
explained_svm <- 0.95 * sum(sorted_weights_svm)
selected_indices_svm <- sorted_indices_svm[cumulative_sum_svm <= explained_svm]
# Step 4: Subset the weights_svm vector based on selected indices
selected_weights_svm <- weights_svm[selected_indices_svm]
num_features_svm_red <- length(selected_weights_svm)
print(num_features_svm_red)
library(ggplot2)
# Initialize an empty data frame to store results
threshold_results_svm <- data.frame(Threshold_svm = numeric(), BalancedAccuracy_svm = numeric())
# Define a sequence of threshold values
threshold_values_svm <- seq(0, 0.5 , by = 0.005)
predictions_svm_valid_list <- predict(final_model_svm_red, sparse_data_valid, proba = T) #CHANGE TEST INTO VALIDATE
# Make predictions on the valid data
predictions_svm_valid_list <- predict(final_model_svm, sparse_data_valid, proba = T) #CHANGE TEST INTO VALIDATE
predictions_svm_valid <- predictions_svm_valid_list$probabilities[, 2]
# Convert probabilities to binary predictions (0 or 1)
predictions_binary_svm <- ifelse(predictions_svm_valid > 0.1, 1, 0)  #INPUT THRESHOLD MANUALLY !!!!
# Evaluate the final model
accuracy_svm <- sum(predictions_binary_svm == y_valid) / length(y_valid)
print(paste("Final Accuracy with all features:", accuracy_svm))
# Confusion matrix
conf_matrix_svm <- table(Predicted = predictions_binary_svm, Actual = y_valid)
print("Confusion Matrix:")
print(conf_matrix_svm)
sparse_data_train_svm <- sparse_data_train[, selected_indices_svm, drop = F]
bestType = 6
final_model_svm <- LiblineaR(data=sparse_data_train,target=y_train,type=bestType,cost=bestCost,bias=1,verbose=FALSE)
weights_svm <- final_model_svm$W
sorted_indices_svm <- order(abs(weights_svm), decreasing = TRUE)
sorted_weights_svm <- abs(weights_svm[sorted_indices_svm])
cumulative_sum_svm <- cumsum(sorted_weights_svm)
explained_svm <- 0.95 * sum(sorted_weights_svm)
selected_indices_svm <- sorted_indices_svm[cumulative_sum_svm <= explained_svm]
# Step 4: Subset the weights_svm vector based on selected indices
selected_weights_svm <- weights_svm[selected_indices_svm]
num_features_svm_red <- length(selected_weights_svm)
print(num_features_svm_red)
sparse_data_train_svm <- sparse_data_train[, selected_indices_svm, drop = F]
library(Matrix)
library(glmnet)
#create dep. var
y_train <- training_data[,1]
y_valid <- validation_data[,1]
y_test <- testing_data[,1]
#create training,  validation and  testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")
features_valid <- validation_data[,-1]
sparse_data_valid <- as(features_valid, "sparseMatrix")
features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")
sparse_data_train_svm <- sparse_data_train[, selected_indices_svm, drop = F]
bestType = 6
final_model_svm <- LiblineaR(data=sparse_data_train,target=y_train,type=bestType,cost=bestCost,bias=1,verbose=FALSE)
weights_svm <- final_model_svm$W
sorted_indices_svm <- order(abs(weights_svm), decreasing = TRUE)
sorted_weights_svm <- abs(weights_svm[sorted_indices_svm])
cumulative_sum_svm <- cumsum(sorted_weights_svm)
explained_svm <- 0.95 * sum(sorted_weights_svm)
selected_indices_svm <- sorted_indices_svm[cumulative_sum_svm <= explained_svm]
# Step 4: Subset the weights_svm vector based on selected indices
selected_weights_svm <- weights_svm[selected_indices_svm]
num_features_svm_red <- length(selected_weights_svm)
print(num_features_svm_red)
sparse_data_train_svm <- sparse_data_train[, selected_indices_svm, drop = F]
library(SparseM)
sparse_data_train_svm <- sparse_data_train[, selected_indices_svm, drop = F]
library(Matrix)
library(glmnet)
#create dep. var
y_train <- training_data[,1]
y_valid <- validation_data[,1]
y_test <- testing_data[,1]
#create training,  validation and  testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")
features_valid <- validation_data[,-1]
sparse_data_valid <- as(features_valid, "sparseMatrix")
features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")
bestType = 6
final_model_svm <- LiblineaR(data=sparse_data_train,target=y_train,type=bestType,cost=bestCost,bias=1,verbose=FALSE)
weights_svm <- final_model_svm$W
sorted_indices_svm <- order(abs(weights_svm), decreasing = TRUE)
sorted_weights_svm <- abs(weights_svm[sorted_indices_svm])
cumulative_sum_svm <- cumsum(sorted_weights_svm)
explained_svm <- 0.95 * sum(sorted_weights_svm)
selected_indices_svm <- sorted_indices_svm[cumulative_sum_svm <= explained_svm]
# Step 4: Subset the weights_svm vector based on selected indices
selected_weights_svm <- weights_svm[selected_indices_svm]
num_features_svm_red <- length(selected_weights_svm)
print(num_features_svm_red)
sparse_data_train_svm <- sparse_data_train[, selected_indices_svm, drop = F]
library(Matrix)
sparse_data_train_svm <- sparse_data_train[, selected_indices_svm, drop = FALSE]
selected_indices_svm
library(Matrix)
df_data_train <- as.data.frame(as.matrix(sparse_data_train))
df_data_train <- as.data.frame(as.matrix(sparse_data_train))
df_data_train_selected <- df_data_train[, selected_indices_svm, drop = FALSE]
library(Matrix)
df_data_train <- as.data.frame(as.matrix(sparse_data_train))
df_data_train_selected <- df_data_train[, selected_indices_svm, drop = FALSE]
head(df_data_train)
max(selected_indices_svm)
data <- read.csv("data2.csv.gz")
# Iterate through label and make them 0 if they are -1, 1 otherwise
for (i in 1:nrow(data)) {
if (data[i, 1] == -1) {
data[i, 1] <- 0
} else {
data[i, 1] <- 1
}
}
# Find duplicate columns
unique_data <- data[, !duplicated(as.list(data))]
num_repeated <- ncol(data) - ncol(unique_data)
cat("The new dataset has dimensions:", dim(unique_data), "\n")
# Calculate row indices for each split
set.seed(12)  # Set seed for reproducibility
unique_data <- unique_data[sample(nrow(unique_data)), ]
n <- nrow(unique_data)
end_training <- floor(n * 0.7)  # 70% for training
end_validation <- floor(n * 0.85)  # Next 10% for validation
# Split the data into training, validation, and testing sets
training_data <- unique_data[1:end_training, ]  # 70% of the data
validation_data <- unique_data[(end_training + 1):end_validation, ]  # 10% of the data
testing_data <- unique_data[(end_validation + 1):n, ]  # Remaining 20% of the data
# # Use 80% of the data for training
# end_training <- floor(nrow(unique_data) * 0.8)
# # Split the data into training and testing sets
# training_data <- unique_data[1:end_training, ]
# testing_data <- unique_data[(end_training + 81):nrow(unique_data), ]
print(sum(training_data[,1]))
print(sum(testing_data[,1]))
print(sum(validation_data[,1]))
library(Matrix)
library(glmnet)
#create dep. var
y_train <- training_data[,1]
y_valid <- validation_data[,1]
y_test <- testing_data[,1]
#create training,  validation and  testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")
features_valid <- validation_data[,-1]
sparse_data_valid <- as(features_valid, "sparseMatrix")
features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")
# Install necessary packages (if not already installed)
if (!require("LiblineaR")) install.packages("LiblineaR")
if (!require("Matrix")) install.packages("Matrix")
# Load libraries
library(LiblineaR)
library(Matrix)
# y_train <- as.factor(y_train)  # Convert to factor if it's classification
tryTypes=c(1,2,3,4,6)
tryCosts=c(1000, 100, 1, 0.1, 0.001)
bestCost=NA
bestAcc=0
bestType=NA
for(ty in tryTypes){
for(co in tryCosts){
acc=LiblineaR(data=sparse_data_train,target=y_train,type=ty,cost=co,bias=1,cross=5,verbose=FALSE)
if(acc>bestAcc){
bestCost=co
bestAcc=acc
bestType=ty
}
}
}
cat("Best model type is:",bestType,"\n")
cat("Best cost is:",bestCost,"\n")
cat("Best accuracy is:",bestAcc,"\n")
bestType = 6
final_model_svm <- LiblineaR(data=sparse_data_train,target=y_train,type=bestType,cost=bestCost,bias=1,verbose=FALSE)
weights_svm <- final_model_svm$W
sorted_indices_svm <- order(abs(weights_svm), decreasing = TRUE)
sorted_weights_svm <- abs(weights_svm[sorted_indices_svm])
cumulative_sum_svm <- cumsum(sorted_weights_svm)
explained_svm <- 0.95 * sum(sorted_weights_svm)
selected_indices_svm <- sorted_indices_svm[cumulative_sum_svm <= explained_svm]
# Step 4: Subset the weights_svm vector based on selected indices
selected_weights_svm <- weights_svm[selected_indices_svm]
num_features_svm_red <- length(selected_weights_svm)
print(num_features_svm_red)
selected_indices_svm
max(selected_indices_svm)
library(Matrix)
df_data_train <- as.data.frame(as.matrix(sparse_data_train))
df_data_train <- as.data.frame(as.matrix(sparse_data_train))
df_data_train_selected <- df_data_train[, selected_indices_svm]
class(selected_indices_svm)
selected_indices_svm
length(selected_indices_svm)
type(selected_indices_svm)
df_data_train
head(df_data_train)
head(df_data_train)
df_data_train[,2]
df_data_train <- as.data.frame(as.matrix(sparse_data_train))
df_data_train <- as.data.frame(as.matrix(sparse_data_train))
df_data_train_selected <- df_data_train[, classselected_indices_svm]
df_data_train_selected <- df_data_train[, selected_indices_svm, drop = F ]
head(df_data_train)
df_data_train_selected <- df_data_train[, selected_indices_svm]
class(selected_indices_svm)
head(df_data_train)
if (any(selected_indices_svm > ncol(sparse_data_train))) {
stop("Error: Some indices in selected_indices_svm exceed the number of columns in sparse_data_train.")
}
if (any(selected_indices_svm > ncol(df_data_train))) {
stop("Error: Some indices in selected_indices_svm exceed the number of columns in sparse_data_train.")
}
