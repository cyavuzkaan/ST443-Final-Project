---
title: "TASK2"
output: html_document
date: "2024-11-13"
---

--- LIBRARIES ---

```{r}
library(data.table)
library(dplyr)
library(ggplot2)
library(caret)
library(ISLR)
```

```{r}
data <- read.csv("data2.csv.gz")
```


```{r}
set.seed(123)
na_counts <- sapply(data, function(x) sum(is.na(x)))
totalna <- sum(na_counts)
print(totalna)

```

```{r}
# Compare Binding vs Non-binding
binding_data <- data[data[, 1] == 1, ]
count <- 0  # Initialize count
for (i in seq_len(nrow(data))) {
  if (data[i, 1] == -1) {
    count <- count + 1
  }
}

binding_count = nrow(data) - count
non_binding_count = count
print(non_binding_count)
ggplot(data, aes(x = data[, 1])) + geom_bar() + labs(x = "Binding Status", y = "Count")

```



```{r}
feature_sum <- colSums(data[, 2:100001])
hist(feature_sum, breaks = 50, main = "Distribution of Feature Sums", xlab = "Sum of binary feature values")
```

```{r}
head(data)
```

```{r}
# Iterate through label and make them 0 if they are -1, 1 otherwise
for (i in 1:nrow(data)) {
  if (data[i, 1] == -1) {
    data[i, 1] <- 0  
  } else {
    data[i, 1] <- 1
  }
}
print(data)
```


```{r}
# Find duplicate columns
unique_data <- data[, !duplicated(as.list(data))]
cat("The new dataset has dimensions:", dim(unique_data), "\n")
```

To compare our models we mesure their performance on unseen data. So we split the dataset in training validation and test data (70,15,15)
```{r}
# Calculate row indices for each split
set.seed(12)
unique_data <- unique_data[sample(nrow(unique_data)),]
n <- nrow(unique_data)
end_training <- floor(n * 0.7)  # 70% for training
end_validation <- floor(n * 0.85)  # Next 15% for validation

# Split the data into training, validation, and testing sets
training_data <- unique_data[1:end_training, ]  # 70% of the data
validation_data <- unique_data[(end_training + 1):end_validation, ]  # 10% of the data
testing_data <- unique_data[(end_validation + 1):n, ]  # Remaining 20% of the data

# Print dimensions of each set
print(dim(training_data))   
print(dim(validation_data))
print(dim(testing_data))

```


Lets build Lasso and Elastic Net with the training data
```{r}
set.seed(12)
library(Matrix)
library(glmnet)

#create dep. var
y_train <- training_data[,1]
y_val <- validation_data[ ,1]
y_true <- testing_data[,1]

#create training, validation and testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")

features_val <- validation_data[,-1]
sparse_data_val <- as(features_val, "sparseMatrix")

features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")


#LASSO CLEAN
cv_fit_1d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1)
selected_features_1d <- coef(cv_fit_1d, s = "lambda.min")
selected_features_1d<- as.matrix(selected_features_1d)
selected_features_nonzero_1d <- selected_features_1d[selected_features_1d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_1d)," features according to LASSO in unique dataset","\n")
print(selected_features_nonzero_1d)
```


```{r}
set.seed(12)
#Elastic Net CLEAN
cv_fit_3d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1/2)
selected_features_3d <- coef(cv_fit_3d, s = "lambda.min")
selected_features_3d<- as.matrix(selected_features_3d)
selected_features_nonzero_3d <- selected_features_3d[selected_features_3d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_3d)," features according to Elastic Net in unique dataset","\n")
print(selected_features_nonzero_3d)
```
	

Plot the choice of lamda
```{r}
## plot of the solution path, i.e. estimated coefficients vs log (lambda), where lambda is the tuning parameter
plot(cv_fit_1d, xvar="lambda", label= TRUE)
plot(cv_fit_3d, xvar="lambda", label= TRUE)
```

We compute the optimal threshold using the validation set and the balance accurancy metric 
```{r} 
library(PRROC)

# Lasso Model Predictions
predictions_L <- predict(cv_fit_1d, sparse_data_val, s = "lambda.min", type = "response")

# Elastic Net Model Predictions
predictions_EN <- predict(cv_fit_3d, sparse_data_val, s = "lambda.min", type = "response")

# Initialize data frames to store results
threshold_results_L <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric())
threshold_results_EN <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric())

# Define a sequence of threshold values
threshold_values <- seq(0, 1, by = 0.001)

# Helper function to calculate balanced accuracy for a given set of predictions and thresholds
calculate_balanced_accuracy_1 <- function(predictions, y_actual, thresholds, model_name) {
  results <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric(), Model = character())
  
  for (threshold in thresholds) {
    # Convert probabilities to binary predictions
    predicted_classes <- ifelse(predictions > threshold, 1, 0)
    
    # Create confusion matrix
    conf_matrix <- table(Predicted = predicted_classes, Actual = y_actual)
    
    # Extract TP, TN, FP, FN (handle missing categories with tryCatch)
    tn <- tryCatch(conf_matrix[1, 1], error = function(e) 0)
    tp <- tryCatch(conf_matrix[2, 2], error = function(e) 0)
    fp <- tryCatch(conf_matrix[2, 1], error = function(e) 0)
    fn <- tryCatch(conf_matrix[1, 2], error = function(e) 0)
    
    # Calculate balanced accuracy
    balanced_acc <- 0.5 * (tp / (tp + fn + 1e-6) + tn / (tn + fp + 1e-6)) # Add small value to avoid division by zero
    
    # Append results
    results <- rbind(results, data.frame(Threshold = threshold, BalancedAccuracy = balanced_acc, Model = model_name))
  }
  return(results)
}

# Calculate balanced accuracy for Lasso and Elastic Net
threshold_results_L <- calculate_balanced_accuracy_1(predictions_L, y_val, threshold_values, "Lasso")
threshold_results_EN <- calculate_balanced_accuracy_1(predictions_EN, y_val, threshold_values, "ElasticNet")

# Combine results for plotting
threshold_results <- rbind(threshold_results_L, threshold_results_EN)

# Plot Balanced Accuracy vs Threshold
library(ggplot2)

ggplot(threshold_results, aes(x = Threshold, y = BalancedAccuracy, color = Model)) +
  geom_line(size = 1) +
  labs(
    title = "Balanced Accuracy vs Threshold",
    x = "Threshold",
    y = "Balanced Accuracy"
  ) +
  theme_minimal()

# Find and print maximum balanced accuracy for each model
max_balanced_accuracy_L <- threshold_results_L[which.max(threshold_results_L$BalancedAccuracy), ]
max_balanced_accuracy_EN <- threshold_results_EN[which.max(threshold_results_EN$BalancedAccuracy), ]

cat("Maximum Balanced Accuracy (Lasso):\n")
cat("Threshold:", max_balanced_accuracy_L$Threshold, "\n")
cat("Balanced Accuracy:", max_balanced_accuracy_L$BalancedAccuracy, "\n")

cat("Maximum Balanced Accuracy (ElasticNet):\n")
cat("Threshold:", max_balanced_accuracy_EN$Threshold, "\n")
cat("Balanced Accuracy:", max_balanced_accuracy_EN$BalancedAccuracy, "\n")


```



```{r}
library(PRROC)

# Predict on the test to see performance 
# Lasso Model Predictions
predictions_1d_test <- predict(cv_fit_1d, sparse_data_test, s = "lambda.min", type = "response")

# Elastic Net Model Predictions (assuming you have cv_fit_3d for Elastic Net)
predictions_3d_test <- predict(cv_fit_3d, sparse_data_test, s = "lambda.min", type = "response")

```


```{r}
# Measure performance on unseen data using the true y
# Convert probabilities to binary predictions using the optimal threshold for each model (tuning the parameters)
predicted_classes_lasso <- ifelse(predictions_1d_test > 0.034, 1, 0)

predicted_classes_EN <- ifelse(predictions_3d_test > 0.029, 1, 0) 
```


```{r}
# Cross validation errors when we were training the models
lasso_errord <- min(cv_fit_1d$cvm)
mix_errord <- min(cv_fit_3d$cvm)

cat("Lasso CV Error:", lasso_errord, "\n")
cat("Elastic CV Net Error:", mix_errord, "\n")
```

```{r}
calculate_balanced_accuracy<- function(conf_matrix) {
  TN <- conf_matrix[1, 1]
  FP <- conf_matrix[2, 1]
  FN <- conf_matrix[1, 2]
  TP <- conf_matrix[2, 2]
  
  sensitivity <- TP / (TP + FN)  # Sensitivity
  specificity <- TN / (TN + FP)  # Specificity

  (sensitivity + specificity) / 2
}
```


```{r}
# Confusion Matrix for Lasso
conf_matrix_lasso <- table(Predicted = predicted_classes_lasso, Actual = y_true)
print("Confusion Matrix Lasso:")
print(conf_matrix_lasso)

# Confusion Matrix for Elastic Net
conf_matrix_EN <- table(Predicted = predicted_classes_EN, Actual = y_true)
print("Confusion Matrix Elastic Net:")
print(conf_matrix_EN)


balanced_accuracy_L <- calculate_balanced_accuracy(conf_matrix_lasso)
balanced_accuracy_EN <- calculate_balanced_accuracy(conf_matrix_EN)

cat("The Balanced Accuracy of Lasso on the test set is:", balanced_accuracy_L, "\n")
cat("The Balanced Accuracy of Elastic Net on the test set is:", balanced_accuracy_EN, "\n")

```

```{r}
#h2o.shutdown(prompt = FALSE)
options(java.parameters = "-Dai.h2o.disable.xgboost=true")
library(h2o)
h2o.init()
```

----------- RANDOM FOREST ---------

```{r}
#h2o.init(max_mem_size = "16G")
train_h2o <- as.h2o(training_data)

train_h2o[, "label"] <- as.factor(train_h2o[, "label"])

x <- colnames(training_data)[-which(names(training_data) == "label")]  # All columns except the target
y <- colnames(training_data[1])

# Train the H2O Random Forest Model
rf_model <- h2o.randomForest(
  x = x,                          # Predictors (column names)
  y = y,                          # Target (name of the target column)
  training_frame = train_h2o,     # Training data
  ntrees = 100,                   # Number of trees
  max_depth = 15,                 # Maximum tree depth
  mtries = 1000,                  # Features considered per split
  min_rows = 20,                  # Minimum rows per leaf
  sample_rate = 0.8,               # Row sampling rate
  seed = 123 
)

# Retrieve variable importance
importance <- h2o.varimp(rf_model)
# Plot variable importance
h2o.varimp_plot(rf_model, num_of_features =75)
head(importance[order(importance$relative_importance, decreasing = TRUE), ], 75)

print(importance)


cumulative_importance85 <- 0
selected_features85 <- c()
for (i in 1:nrow(importance)) {
  cumulative_importance85 <- cumulative_importance85 + importance$percentage[i]
  selected_features85 <- c(selected_features85, importance$variable[i])
  if (cumulative_importance85 >= 0.85) break
}
length(selected_features85)

cumulative_importance90 <- 0
selected_features90 <- c()
for (i in 1:nrow(importance)) {
  cumulative_importance90 <- cumulative_importance90 + importance$percentage[i]
  selected_features90 <- c(selected_features90, importance$variable[i])
  if (cumulative_importance90 >= 0.9) break
}
length(selected_features90)

cumulative_importance95 <- 0
selected_features95 <- c()
for (i in 1:nrow(importance)) {
  cumulative_importance95 <- cumulative_importance95 + importance$percentage[i]
  selected_features95 <- c(selected_features95, importance$variable[i])
  if (cumulative_importance95 >= 0.95) break
}
length(selected_features95)

selected_features100 <- importance[importance$scaled_importance > 0, "variable"]
length(selected_features100)
```

```{r}
rf_model_selected_85 <- h2o.randomForest(
  x = selected_features85,
  y = y,
  training_frame = train_h2o,
  ntrees = 75,               # Fewer trees for small datasets
  max_depth = 10,             # Restrict tree depth to avoid overfitting
  mtries = 10,               # Use sqrt(number of features) for classification
  min_rows = 5,              # Smaller minimum rows for better splits
  sample_rate = 0.9,          # Slightly higher sampling rate for diversity
  seed = 123 
)


val_h2o <- as.h2o(validation_data[,-1])
predictions_rf <- (h2o.predict(rf_model_selected_85, val_h2o))
predictions_rf <- as.data.frame(predictions_rf)
predictions_rf <- as.numeric(predictions_rf$p1)  

# Initialize data frames to store results
threshold_results_rf <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric())

# Define a sequence of threshold values
threshold_values <- seq(0, 1, by = 0.001)

# Helper function to calculate balanced accuracy for a given set of predictions and thresholds
calculate_balanced_accuracy_1 <- function(predictions, y_actual, thresholds, model_name) {
  results <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric(), Model = character())
  
  for (threshold in thresholds) {
    # Convert probabilities to binary predictions
    predicted_classes <- ifelse(predictions > threshold, 1, 0)
    
    # Create confusion matrix
    conf_matrix <- table(Predicted = predicted_classes, Actual = y_actual)
    
    # Extract TP, TN, FP, FN (handle missing categories with tryCatch)
    tn <- tryCatch(conf_matrix[1, 1], error = function(e) 0)
    tp <- tryCatch(conf_matrix[2, 2], error = function(e) 0)
    fp <- tryCatch(conf_matrix[2, 1], error = function(e) 0)
    fn <- tryCatch(conf_matrix[1, 2], error = function(e) 0)
    
    # Calculate balanced accuracy
    balanced_acc <- 0.5 * (tp / (tp + fn + 1e-6) + tn / (tn + fp + 1e-6)) # Add small value to avoid division by zero
    
    # Append results
    results <- rbind(results, data.frame(Threshold = threshold, BalancedAccuracy = balanced_acc, Model = model_name))
  }
  return(results)
}

# Calculate balanced accuracy
threshold_results_rf_85<- calculate_balanced_accuracy_1(predictions_rf, y_val, threshold_values, "Random Forest")

# Find and print maximum balanced accuracy for each model
max_balanced_accuracy_rf_85 <- threshold_results_rf_85[which.max(threshold_results_rf_85$BalancedAccuracy), ]

cat(" (Random Forest (85) on Validation set):\n")
cat("Threshold:", max_balanced_accuracy_rf_85$Threshold, "\n")
cat("Balanced Accuracy:", max_balanced_accuracy_rf_85$BalancedAccuracy, "\n")


```


```{r}
rf_model_selected_90 <- h2o.randomForest(
  x = selected_features90,
  y = y,
  training_frame = train_h2o,
  ntrees = 75,               # Fewer trees for small datasets
  max_depth = 10,             # Restrict tree depth to avoid overfitting
  mtries = 10,               # Use sqrt(number of features) for classification
  min_rows = 5,              # Smaller minimum rows for better splits
  sample_rate = 0.9,          # Slightly higher sampling rate for diversity
  seed = 123 
)


val_h2o <- as.h2o(validation_data[,-1])
predictions_rf <- (h2o.predict(rf_model_selected_90, val_h2o))
predictions_rf <- as.data.frame(predictions_rf)
predictions_rf <- as.numeric(predictions_rf$p1)  

# Initialize data frames to store results
threshold_results_rf <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric())

# Define a sequence of threshold values
threshold_values <- seq(0, 1, by = 0.001)

# Helper function to calculate balanced accuracy for a given set of predictions and thresholds
calculate_balanced_accuracy_1 <- function(predictions, y_actual, thresholds, model_name) {
  results <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric(), Model = character())
  
  for (threshold in thresholds) {
    # Convert probabilities to binary predictions
    predicted_classes <- ifelse(predictions > threshold, 1, 0)
    
    # Create confusion matrix
    conf_matrix <- table(Predicted = predicted_classes, Actual = y_actual)
    
    # Extract TP, TN, FP, FN (handle missing categories with tryCatch)
    tn <- tryCatch(conf_matrix[1, 1], error = function(e) 0)
    tp <- tryCatch(conf_matrix[2, 2], error = function(e) 0)
    fp <- tryCatch(conf_matrix[2, 1], error = function(e) 0)
    fn <- tryCatch(conf_matrix[1, 2], error = function(e) 0)
    
    # Calculate balanced accuracy
    balanced_acc <- 0.5 * (tp / (tp + fn + 1e-6) + tn / (tn + fp + 1e-6)) # Add small value to avoid division by zero
    
    # Append results
    results <- rbind(results, data.frame(Threshold = threshold, BalancedAccuracy = balanced_acc, Model = model_name))
  }
  return(results)
}

# Calculate balanced accuracy
threshold_results_rf_90<- calculate_balanced_accuracy_1(predictions_rf, y_val, threshold_values, "Random Forest")

# Find and print maximum balanced accuracy for each model
max_balanced_accuracy_rf_90 <- threshold_results_rf_90[which.max(threshold_results_rf_90$BalancedAccuracy), ]

cat(" (Random Forest (90) on Validatio set:\n")
cat("Threshold:", max_balanced_accuracy_rf_90$Threshold, "\n")
cat("Balanced Accuracy:", max_balanced_accuracy_rf_90$BalancedAccuracy, "\n")


```


```{r}
set.seed(123)
rf_model_selected_95 <- h2o.randomForest(
  x = selected_features95,
  y = y,
  training_frame = train_h2o,
  ntrees = 75,               # Fewer trees for small datasets
  max_depth = 10,             # Restrict tree depth to avoid overfitting
  mtries = 10,               # Use sqrt(number of features) for classification
  min_rows = 5,              # Smaller minimum rows for better splits
  sample_rate = 0.9,          # Slightly higher sampling rate for diversity
  seed = 123 
)


val_h2o <- as.h2o(validation_data[,-1])
predictions_rf <- (h2o.predict(rf_model_selected_95, val_h2o))
predictions_rf <- as.data.frame(predictions_rf)
predictions_rf <- as.numeric(predictions_rf$p1)  

# Initialize data frames to store results
threshold_results_rf <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric())

# Define a sequence of threshold values
threshold_values <- seq(0, 1, by = 0.001)

# Helper function to calculate balanced accuracy for a given set of predictions and thresholds
calculate_balanced_accuracy_1 <- function(predictions, y_actual, thresholds, model_name) {
  results <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric(), Model = character())
  
  for (threshold in thresholds) {
    # Convert probabilities to binary predictions
    predicted_classes <- ifelse(predictions > threshold, 1, 0)
    
    # Create confusion matrix
    conf_matrix <- table(Predicted = predicted_classes, Actual = y_actual)
    
    # Extract TP, TN, FP, FN (handle missing categories with tryCatch)
    tn <- tryCatch(conf_matrix[1, 1], error = function(e) 0)
    tp <- tryCatch(conf_matrix[2, 2], error = function(e) 0)
    fp <- tryCatch(conf_matrix[2, 1], error = function(e) 0)
    fn <- tryCatch(conf_matrix[1, 2], error = function(e) 0)
    
    # Calculate balanced accuracy
    balanced_acc <- 0.5 * (tp / (tp + fn + 1e-6) + tn / (tn + fp + 1e-6)) # Add small value to avoid division by zero
    
    # Append results
    results <- rbind(results, data.frame(Threshold = threshold, BalancedAccuracy = balanced_acc, Model = model_name))
  }
  return(results)
}

# Calculate balanced accuracy
threshold_results_rf_95<- calculate_balanced_accuracy_1(predictions_rf, y_val, threshold_values, "Random Forest")


# Find and print maximum balanced accuracy for each model
max_balanced_accuracy_rf_95 <- threshold_results_rf_95[which.max(threshold_results_rf_95$BalancedAccuracy), ]

cat(" (Random Forest (95) on Validation set):\n")
cat("Threshold:", max_balanced_accuracy_rf_95$Threshold, "\n")
cat("Balanced Accuracy:", max_balanced_accuracy_rf_95$BalancedAccuracy, "\n")

```


```{r}
rf_model_selected_100 <- h2o.randomForest(
  x = selected_features100,
  y = y,
  training_frame = train_h2o,
  ntrees = 75,               # Fewer trees for small datasets
  max_depth = 10,             # Restrict tree depth to avoid overfitting
  mtries = 10,               # Use sqrt(number of features) for classification
  min_rows = 5,              # Smaller minimum rows for better splits
  sample_rate = 0.9,          # Slightly higher sampling rate for diversity
  seed = 123 
)


val_h2o <- as.h2o(validation_data[,-1])
predictions_rf <- (h2o.predict(rf_model_selected_100, val_h2o))
predictions_rf <- as.data.frame(predictions_rf)
predictions_rf <- as.numeric(predictions_rf$p1)  

# Initialize data frames to store results
threshold_results_rf <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric())

# Define a sequence of threshold values
threshold_values <- seq(0, 1, by = 0.001)

# Helper function to calculate balanced accuracy for a given set of predictions and thresholds
calculate_balanced_accuracy_1 <- function(predictions, y_actual, thresholds, model_name) {
  results <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric(), Model = character())
  
  for (threshold in thresholds) {
    # Convert probabilities to binary predictions
    predicted_classes <- ifelse(predictions > threshold, 1, 0)
    
    # Create confusion matrix
    conf_matrix <- table(Predicted = predicted_classes, Actual = y_actual)
    
    # Extract TP, TN, FP, FN (handle missing categories with tryCatch)
    tn <- tryCatch(conf_matrix[1, 1], error = function(e) 0)
    tp <- tryCatch(conf_matrix[2, 2], error = function(e) 0)
    fp <- tryCatch(conf_matrix[2, 1], error = function(e) 0)
    fn <- tryCatch(conf_matrix[1, 2], error = function(e) 0)
    
    # Calculate balanced accuracy
    balanced_acc <- 0.5 * (tp / (tp + fn + 1e-6) + tn / (tn + fp + 1e-6)) # Add small value to avoid division by zero
    
    # Append results
    results <- rbind(results, data.frame(Threshold = threshold, BalancedAccuracy = balanced_acc, Model = model_name))
  }
  return(results)
}

# Calculate balanced accuracy
threshold_results_rf_100<- calculate_balanced_accuracy_1(predictions_rf, y_val, threshold_values, "Random Forest")

# Find and print maximum balanced accuracy for each model
max_balanced_accuracy_rf_100 <- threshold_results_rf_90[which.max(threshold_results_rf_100$BalancedAccuracy), ]

cat(" (Random Forest (100) on Validatio set:\n")
cat("Threshold:", max_balanced_accuracy_rf_100$Threshold, "\n")
cat("Balanced Accuracy:", max_balanced_accuracy_rf_100$BalancedAccuracy, "\n")

```



Summarize for the different variables= amount
```{r}
# Combine results for plotting balanced accuracy over thresholds on validation set
threshold_results_rf_85$Model <- "RF (85%)"
threshold_results_rf_90$Model <- "RF (90%)"
threshold_results_rf_95$Model <- "RF (95%)"
threshold_results_rf_100$Model <- "RF (100%)"

threshold_results <- rbind(threshold_results_rf_85, threshold_results_rf_90, threshold_results_rf_95, threshold_results_rf_100)


# Plot Balanced Accuracy vs Threshold
ggplot(threshold_results, aes(x = Threshold, y = BalancedAccuracy, color= Model)) +
  geom_line(size = 1) +
  labs(
    title = "Balanced Accuracy vs Threshold",
    x = "Threshold",
    y = "Balanced Accuracy"
  ) +
  theme_minimal()

model_results_rf <- data.frame(
  Model = character(),
  BalancedAccuracy = numeric(),
  NumFeatures = numeric()
)

# Performance on validation set
model_results_rf <- rbind(model_results_rf, data.frame(Model = "Random Forest with 85% importance features", BalancedAccuracy = max_balanced_accuracy_rf_85$BalancedAccuracy, NumFeatures = length(selected_features85)))
model_results_rf <- rbind(model_results_rf, data.frame(Model = "Random Forest with 90% importance features", BalancedAccuracy = max_balanced_accuracy_rf_90$BalancedAccuracy, NumFeatures = length(selected_features90)))
model_results_rf <- rbind(model_results_rf, data.frame(Model = "Random Forest with 95% importance features", BalancedAccuracy = max_balanced_accuracy_rf_95$BalancedAccuracy, NumFeatures = length(selected_features95)))
model_results_rf <- rbind(model_results_rf, data.frame(Model = "Random Forest with 100% importance features", BalancedAccuracy = max_balanced_accuracy_rf_100$BalancedAccuracy, NumFeatures = length(selected_features100)))

# View the updated data frame
print(model_results_rf)

```
```{r}

# Make predictions on the test data to mesure performance
test_h2o <- as.h2o(testing_data[,-1])
predictions_rf90 <- (h2o.predict(rf_model_selected_90, test_h2o))
predictions_rf90 <- as.data.frame(predictions_rf90)
predictions_rf90 <- as.numeric(predictions_rf90$p1)  

predicted_classes_RF_90 <- ifelse(predictions_rf90 > max_balanced_accuracy_rf_90$Threshold, 1, 0) #Using optimal threshold

conf_matrix_RF_90 <- table(Predicted =  predicted_classes_RF_90 , Actual = y_true)
print("Confusion Matrix Random Forest (90):")
print(conf_matrix_RF_90)

balanced_accuracy_RF_90 <- calculate_balanced_accuracy(conf_matrix_RF_90)
cat("The Balanced Accuracy of Random Forest (90) on the test set is:", balanced_accuracy_RF_90, "\n")

```


```{r}
# Initialize an empty data frame
model_results <- data.frame(
  Model = character(),
  BalancedAccuracy = numeric(),
  NumFeatures = numeric()
)

# Add results dynamically
model_results <- rbind(model_results, data.frame(Model = "Lasso", BalancedAccuracy = balanced_accuracy_L, NumFeatures = length(selected_features_nonzero_1d)))
model_results <- rbind(model_results, data.frame(Model = "Elastic Net", BalancedAccuracy = balanced_accuracy_EN, NumFeatures = length(selected_features_nonzero_3d)))
model_results <- rbind(model_results, data.frame(Model = "Random Forest (Optimal)", BalancedAccuracy = balanced_accuracy_RF_90, NumFeatures = length(selected_features90)))

# View the updated data frame
print(model_results)
```



```{r}

# Convert training and validation data to H2O frames
train_h2o <- as.h2o(training_data)
val_h2o <- as.h2o(validation_data)
train_h2o[, y] <- as.factor(train_h2o[, y])  # Convert the response column in training set to factor
val_h2o[, y] <- as.factor(val_h2o[, y])      # Convert the response column in validation set to factor

# Ensure the response column is correctly set
train_h2o[, "label"] <- as.factor(train_h2o[, "label"])  # Convert to factor for classification
val_h2o[, "label"] <- as.factor(val_h2o[, "label"])      # Convert to factor for classification

# Define predictors and response
x <- setdiff(h2o.colnames(train_h2o), "label")  # All columns except the target
y <- "label"

set.seed(123)
# Train the H2O GBM model
gbm_model <- h2o.gbm(
  x = x,
  y = y,
  training_frame = train_h2o,
  validation_frame = val_h2o,
  ntrees = 100,
  max_depth = 8,
  learn_rate = 0.05,
  sample_rate = 0.8,
  col_sample_rate = 0.8,
  stopping_rounds = 5,
  stopping_metric = "AUC"
)

# Retrieve variable importance
importance_gbm <- h2o.varimp(gbm_model)
importance_gbm
# Plot variable importance
h2o.varimp_plot(gbm_model, num_of_features = 5) 

cumulative_importance_gbm <- 0
selected_features_gbm <- c()
for (i in 1:nrow(importance_gbm)) {
  cumulative_importance_gbm <- cumulative_importance_gbm + importance_gbm$percentage[i]
  selected_features_gbm <- c(selected_features_gbm, importance_gbm$variable[i])
  if (cumulative_importance_gbm >= 0.95) break
}
print(length(selected_features_gbm))


gbm_model_selected <- h2o.gbm(
  x = selected_features_gbm,         # Only the selected features
  y = y,                         # Response variable
  training_frame = train_h2o,    # Original training data
  validation_frame = val_h2o,    # Original validation data
  ntrees = 100,
  max_depth = 10,
  learn_rate = 0.005,
  sample_rate = 0.3,
  col_sample_rate = 0.3,
  stopping_rounds = 5,
  stopping_metric = "AUC"
)


predictions_gbm <- (h2o.predict(gbm_model_selected, val_h2o))
predictions_gbm <- as.data.frame(predictions_gbm)
predictions_gbm <- as.numeric(predictions_gbm$p1)  

# Initialize data frames to store results
threshold_results_gbm <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric())

# Define a sequence of threshold values
threshold_values <- seq(0, 1, by = 0.001)

# Helper function to calculate balanced accuracy for a given set of predictions and thresholds
calculate_balanced_accuracy_1 <- function(predictions, y_actual, thresholds, model_name) {
  results <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric(), Model = character())
  
  for (threshold in thresholds) {
    # Convert probabilities to binary predictions
    predicted_classes <- ifelse(predictions > threshold, 1, 0)
    
    # Create confusion matrix
    conf_matrix <- table(Predicted = predicted_classes, Actual = y_actual)
    
    # Extract TP, TN, FP, FN (handle missing categories with tryCatch)
    tn <- tryCatch(conf_matrix[1, 1], error = function(e) 0)
    tp <- tryCatch(conf_matrix[2, 2], error = function(e) 0)
    fp <- tryCatch(conf_matrix[2, 1], error = function(e) 0)
    fn <- tryCatch(conf_matrix[1, 2], error = function(e) 0)
    
    # Calculate balanced accuracy
    balanced_acc <- 0.5 * (tp / (tp + fn + 1e-6) + tn / (tn + fp + 1e-6)) # Add small value to avoid division by zero
    
    # Append results
    results <- rbind(results, data.frame(Threshold = threshold, BalancedAccuracy = balanced_acc, Model = model_name))
  }
  return(results)
}

# Calculate balanced accuracy for Lasso and Elastic Net
threshold_results_gbm<- calculate_balanced_accuracy_1(predictions_gbm, y_val, threshold_values, "Gradient boosting method")

# Combine results for plotting
threshold_results <- rbind(threshold_results_gbm) #, threshold_results_rf, threshold_results_EN, threshold_results_L)


# Plot Balanced Accuracy vs Threshold

ggplot(threshold_results, aes(x = Threshold, y = BalancedAccuracy, color= Model)) +
  geom_line(size = 1) +
  labs(
    title = "Balanced Accuracy vs Threshold",
    x = "Threshold",
    y = "Balanced Accuracy"
  ) +
  theme_minimal()

# Find and print maximum balanced accuracy for each model
max_balanced_accuracy_gbm <- threshold_results_gbm[which.max(threshold_results_gbm$BalancedAccuracy), ]

cat(" (Gradient Boosting Methods):\n")
cat("Threshold:", max_balanced_accuracy_gbm$Threshold, "\n")
cat("Balanced Accuracy:", max_balanced_accuracy_gbm$BalancedAccuracy, "\n")


test_h2o <- as.h2o(testing_data[,-1])

predictions_gbm <- (h2o.predict(gbm_model_selected, test_h2o))
predictions_gbm <- as.data.frame(predictions_gbm)
predictions_gbm <- as.numeric(predictions_gbm$p1)  

predicted_classes_gbm <- ifelse(predictions_gbm > 0.082, 1, 0) #Using optimal threshold

conf_matrix_gbm <- table(Predicted =  predicted_classes_gbm , Actual = y_true)
print("Confusion Matrix Gradient Boosting Method:")
print(conf_matrix_gbm)


#cat("The Balanced Accuracy of Lasso on the test set is:", balanced_accuracy_L, "\n")
#cat("The Balanced Accuracy of Elastic Net on the test set is:", balanced_accuracy_EN, "\n")
#cat("The Balanced Accuracy of Random Forest on the test set is:", balanced_accuracy_RF, "\n")
balanced_accuracy_gbm <- calculate_balanced_accuracy(conf_matrix_gbm)
cat("The Balanced Accuracy of Gradient Boosting Method on the test set is:", balanced_accuracy_gbm, "\n")
```


```{r}
# Initialize an empty data frame
model_results <- data.frame(
  Model = character(),
  BalancedAccuracy = numeric(),
  NumFeatures = numeric()
)

# Add results dynamically
model_results <- rbind(model_results, data.frame(Model = "Lasso", BalancedAccuracy = balanced_accuracy_L, NumFeatures = length(selected_features_nonzero_1d)))
model_results <- rbind(model_results, data.frame(Model = "Elastic Net", BalancedAccuracy = balanced_accuracy_EN, NumFeatures = length(selected_features_nonzero_3d)))
model_results <- rbind(model_results, data.frame(Model = "Random Forest", BalancedAccuracy = balanced_accuracy_RF, NumFeatures = length(selected_features)))
model_results <- rbind(model_results, data.frame(Model = "Gradient Boosting Method", BalancedAccuracy = balanced_accuracy_gbm, NumFeatures =  length(selected_features_gbm)))

# View the updated data frame
print(model_results)
```



If we choose for random forest to have the same amount of features as Lasso or elastic net, we get:
```{r}
set.seed(123)
selected_features_34 <- c()
for (i in 1:34) {
  selected_features_34 <- c(selected_features_34, importance$variable[i])
}
print(selected_features_34)

length(selected_features_34)

rf_model_selected_34 <- h2o.randomForest(
  x = selected_features_34,
  y = y,
  training_frame = train_h2o,
  ntrees = 70,               # Fewer trees to prevent overfitting
  max_depth = 8,             # Restrict tree depth for better generalization
  mtries = -1,               # Feature subsampling
  min_rows = 5,              # Allow finer splits
  sample_rate = 0.8          # Retain 80% row sampling
)


library(PRROC)

val_h2o <- as.h2o(validation_data[,-1])
predictions_rf_34 <- (h2o.predict(rf_model_selected_34, val_h2o))
predictions_rf_34 <- as.data.frame(predictions_rf_34)
predictions_rf_34 <- as.numeric(predictions_rf_34$p1)  

# Initialize data frames to store results
threshold_results_rf <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric())

# Define a sequence of threshold values
threshold_values <- seq(0, 1, by = 0.001)

# Helper function to calculate balanced accuracy for a given set of predictions and thresholds
calculate_balanced_accuracy_1 <- function(predictions, y_actual, thresholds, model_name) {
  results <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric(), Model = character())
  
  for (threshold in thresholds) {
    # Convert probabilities to binary predictions
    predicted_classes <- ifelse(predictions > threshold, 1, 0)
    
    # Create confusion matrix
    conf_matrix <- table(Predicted = predicted_classes, Actual = y_actual)
    
    # Extract TP, TN, FP, FN (handle missing categories with tryCatch)
    tn <- tryCatch(conf_matrix[1, 1], error = function(e) 0)
    tp <- tryCatch(conf_matrix[2, 2], error = function(e) 0)
    fp <- tryCatch(conf_matrix[2, 1], error = function(e) 0)
    fn <- tryCatch(conf_matrix[1, 2], error = function(e) 0)
    
    # Calculate balanced accuracy
    balanced_acc <- 0.5 * (tp / (tp + fn + 1e-6) + tn / (tn + fp + 1e-6)) 
    
    # Append results
    results <- rbind(results, data.frame(Threshold = threshold, BalancedAccuracy = balanced_acc, Model = model_name))
  }
  return(results)
}

threshold_results_rf_34<- calculate_balanced_accuracy_1(predictions_rf_34, y_val, threshold_values, "Random Forest")

# Combine results for plotting
threshold_results_34 <- rbind(threshold_results_rf_34, threshold_results_L)


# Plot Balanced Accuracy vs Threshold

ggplot(threshold_results_34, aes(x = Threshold, y = BalancedAccuracy, color= Model)) +
  geom_line(size = 1) +
  labs(
    title = "Balanced Accuracy vs Threshold",
    x = "Threshold",
    y = "Balanced Accuracy"
  ) +
  theme_minimal()

# Find and print maximum balanced accuracy for each model
max_balanced_accuracy_rf_34 <- threshold_results_34[which.max(threshold_results_rf_34$BalancedAccuracy), ]

cat("Random Forest with the same amount of featurues as lasso")
cat("Threshold:", max_balanced_accuracy_rf_34$Threshold, "\n")
cat("Balanced Accuracy:", max_balanced_accuracy_rf_34$BalancedAccuracy, "\n")

predictions_rf_34 <- (h2o.predict(rf_model_selected_34, test_h2o))
predictions_rf_34 <- as.data.frame(predictions_rf_34)
predictions_rf_34 <- as.numeric(predictions_rf_34$p1)  

predicted_classes_RF_34 <- ifelse(predictions_rf_34 > 0.104, 1, 0)               #Using optimal threshold
conf_matrix_RF_34 <- table(Predicted =  predicted_classes_RF_34 , Actual = y_true)
print("Confusion Matrix Random Forest:")
print(conf_matrix_RF_34)

balanced_accuracy_RF_34 <- calculate_balanced_accuracy(conf_matrix_RF_34)
cat("The Balanced Accuracy of Random Forest on the test set is:", balanced_accuracy_RF_34, "\n")
cat("The Balanced Accuracy of Lasso with 34 features on the test set is:", balanced_accuracy_L, "\n")
```




