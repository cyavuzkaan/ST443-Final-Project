---
title: "TASK2"
output: html_document
date: "2024-11-13"
---

--- LIBRARIES ---

```{r}
library(data.table)
library(dplyr)
library(ggplot2)
library(caret)
library(ISLR)
```

```{r}
data <- read.csv("data2.csv.gz")
```

Binding: Indicates that the compound interacts or attaches effectively to the thrombin target site.(1)
Non-binding: Indicates that the compound does not interact effectively with thrombin.(0)
In the context of drug discovery, thrombin is a protein that could be a target for drugs aimed at affecting blood clotting, and identifying compounds that bind to thrombin can be a critical step in developing new medications.

```{r}
set.seed(123)
na_counts <- sapply(data, function(x) sum(is.na(x)))
totalna <- sum(na_counts)
print(totalna)

```

```{r}
# Compare Binding vs Non-binding
binding_data <- data[data[, 1] == 1, ]
count <- 0  # Initialize count
for (i in seq_len(nrow(data))) {
  if (data[i, 1] == -1) {
    count <- count + 1
  }
}

binding_count = nrow(data) - count
non_binding_count = count
print(non_binding_count)
ggplot(data, aes(x = data[, 1])) + geom_bar() + labs(x = "Binding Status", y = "Count")

```



```{r}
feature_sum <- colSums(data[, 2:100001])
hist(feature_sum, breaks = 50, main = "Distribution of Feature Sums", xlab = "Sum of binary feature values")

```

```{r}
head(data)
```

```{r}
# Iterate through label and make them 0 if they are -1, 1 otherwise
for (i in 1:nrow(data)) {
  if (data[i, 1] == -1) {
    data[i, 1] <- 0  
  } else {
    data[i, 1] <- 1
  }
}
print(data)
```


```{r}
# Find duplicate columns
unique_data <- data[, !duplicated(as.list(data))]
num_repeated <- ncol(data) - ncol(unique_data)
cat("The new dataset has dimensions:", dim(unique_data), "\n")
```

To compare our models we mesure their performance on unseen data. So we split the dataset in training and test
```{r}
# Calculate row indices for each split
set.seed(12)
unique_data <- unique_data[sample(nrow(unique_data)),]
n <- nrow(unique_data)
end_training <- floor(n * 0.7)  # 70% for training
end_validation <- floor(n * 0.85)  # Next 15% for validation

# Split the data into training, validation, and testing sets
training_data <- unique_data[1:end_training, ]  # 70% of the data
validation_data <- unique_data[(end_training + 1):end_validation, ]  # 10% of the data
testing_data <- unique_data[(end_validation + 1):n, ]  # Remaining 20% of the data

# Print dimensions of each set
print(dim(training_data))   
print(dim(validation_data))
print(dim(testing_data))

```


Lets build Lasso and Elastic Net with the training data
```{r}
set.seed(12)
library(Matrix)
library(glmnet)

#create dep. var
y_train <- training_data[,1]
y_val <- validation_data[ ,1]
y_true <- testing_data[,1]

#create training, validation and testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")

features_val <- validation_data[,-1]
sparse_data_val <- as(features_val, "sparseMatrix")

features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")


#LASSO CLEAN
cv_fit_1d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1)
selected_features_1d <- coef(cv_fit_1d, s = "lambda.min")
selected_features_1d<- as.matrix(selected_features_1d)
selected_features_nonzero_1d <- selected_features_1d[selected_features_1d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_1d)," features according to LASSO in unique dataset","\n")
print(selected_features_nonzero_1d)
```


```{r}
set.seed(12)
#Elastic Net CLEAN
cv_fit_3d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1/2)
selected_features_3d <- coef(cv_fit_3d, s = "lambda.min")
selected_features_3d<- as.matrix(selected_features_3d)
selected_features_nonzero_3d <- selected_features_3d[selected_features_3d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_3d)," features according to Elastic Net in unique dataset","\n")
print(selected_features_nonzero_3d)
```
	

Plot the choice of lamda
```{r}
## plot of the solution path, i.e. estimated coefficients vs log (lambda), where lambda is the tuning parameter
plot(cv_fit_1d, xvar="lambda", label= TRUE)
plot(cv_fit_3d, xvar="lambda", label= TRUE)
```
I commented the ridge regression out of the code because it keeps all the features and just makes the coefficeints smaller while the other models actually removes features by shrinking they coefficeints to 0.

We compute the optimal threshold using the validation set ny computing PrAuc and F1
```{r} 
library(PRROC)

# Lasso Model Predictions
predictions_L <- predict(cv_fit_1d, sparse_data_val, s = "lambda.min", type = "response")

# Elastic Net Model Predictions
predictions_EN <- predict(cv_fit_3d, sparse_data_val, s = "lambda.min", type = "response")

# Initialize data frames to store results
threshold_results_L <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric())
threshold_results_EN <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric())

# Define a sequence of threshold values
threshold_values <- seq(0, 1, by = 0.001)

# Helper function to calculate balanced accuracy for a given set of predictions and thresholds
calculate_balanced_accuracy_1 <- function(predictions, y_actual, thresholds, model_name) {
  results <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric(), Model = character())
  
  for (threshold in thresholds) {
    # Convert probabilities to binary predictions
    predicted_classes <- ifelse(predictions > threshold, 1, 0)
    
    # Create confusion matrix
    conf_matrix <- table(Predicted = predicted_classes, Actual = y_actual)
    
    # Extract TP, TN, FP, FN (handle missing categories with tryCatch)
    tn <- tryCatch(conf_matrix[1, 1], error = function(e) 0)
    tp <- tryCatch(conf_matrix[2, 2], error = function(e) 0)
    fp <- tryCatch(conf_matrix[2, 1], error = function(e) 0)
    fn <- tryCatch(conf_matrix[1, 2], error = function(e) 0)
    
    # Calculate balanced accuracy
    balanced_acc <- 0.5 * (tp / (tp + fn + 1e-6) + tn / (tn + fp + 1e-6)) # Add small value to avoid division by zero
    
    # Append results
    results <- rbind(results, data.frame(Threshold = threshold, BalancedAccuracy = balanced_acc, Model = model_name))
  }
  return(results)
}

# Calculate balanced accuracy for Lasso and Elastic Net
threshold_results_L <- calculate_balanced_accuracy_1(predictions_L, y_val, threshold_values, "Lasso")
threshold_results_EN <- calculate_balanced_accuracy_1(predictions_EN, y_val, threshold_values, "ElasticNet")

# Combine results for plotting
threshold_results <- rbind(threshold_results_L, threshold_results_EN)

# Plot Balanced Accuracy vs Threshold
library(ggplot2)

ggplot(threshold_results, aes(x = Threshold, y = BalancedAccuracy, color = Model)) +
  geom_line(size = 1) +
  labs(
    title = "Balanced Accuracy vs Threshold",
    x = "Threshold",
    y = "Balanced Accuracy"
  ) +
  theme_minimal()

# Find and print maximum balanced accuracy for each model
max_balanced_accuracy_L <- threshold_results_L[which.max(threshold_results_L$BalancedAccuracy), ]
max_balanced_accuracy_EN <- threshold_results_EN[which.max(threshold_results_EN$BalancedAccuracy), ]

cat("Maximum Balanced Accuracy (Lasso):\n")
cat("Threshold:", max_balanced_accuracy_L$Threshold, "\n")
cat("Balanced Accuracy:", max_balanced_accuracy_L$BalancedAccuracy, "\n")

cat("Maximum Balanced Accuracy (ElasticNet):\n")
cat("Threshold:", max_balanced_accuracy_EN$Threshold, "\n")
cat("Balanced Accuracy:", max_balanced_accuracy_EN$BalancedAccuracy, "\n")


```



```{r}
library(PRROC)

# Predict on the test to see performance 
# Lasso Model Predictions
predictions_1d_test <- predict(cv_fit_1d, sparse_data_test, s = "lambda.min", type = "response")

# Elastic Net Model Predictions (assuming you have cv_fit_3d for Elastic Net)
predictions_3d_test <- predict(cv_fit_3d, sparse_data_test, s = "lambda.min", type = "response")

```


```{r}
# Measure performance on unseen data using the true y
# Convert probabilities to binary predictions using the optimal threshold for each model (tuning the parameters)
# Inthreshold
predicted_classes_lasso <- ifelse(predictions_1d_test > 0.034, 1, 0)

predicted_classes_EN <- ifelse(predictions_3d_test > 0.029, 1, 0) 
```


```{r}
# Cross validation errors when we were training the models
lasso_errord <- min(cv_fit_1d$cvm)
mix_errord <- min(cv_fit_3d$cvm)

cat("Lasso CV Error:", lasso_errord, "\n")
cat("Elastic CV Net Error:", mix_errord, "\n")
```

```{r}
calculate_balanced_accuracy<- function(conf_matrix) {
  TN <- conf_matrix[1, 1]
  FP <- conf_matrix[2, 1]
  FN <- conf_matrix[1, 2]
  TP <- conf_matrix[2, 2]
  
  sensitivity <- TP / (TP + FN)  # Sensitivity
  specificity <- TN / (TN + FP)  # Specificity

  (sensitivity + specificity) / 2
}
```


```{r}
# Confusion Matrix for Lasso
conf_matrix_lasso <- table(Predicted = predicted_classes_lasso, Actual = y_true)
print("Confusion Matrix Lasso:")
print(conf_matrix_lasso)

# Confusion Matrix for Elastic Net
conf_matrix_EN <- table(Predicted = predicted_classes_EN, Actual = y_true)
print("Confusion Matrix Elastic Net:")
print(conf_matrix_EN)


balanced_accuracy_L <- calculate_balanced_accuracy(conf_matrix_lasso)
balanced_accuracy_EN <- calculate_balanced_accuracy(conf_matrix_EN)

cat("The Balanced Accuracy of Lasso on the test set is:", balanced_accuracy_L, "\n")
cat("The Balanced Accuracy of Elastic Net on the test set is:", balanced_accuracy_EN, "\n")

```

```{r}
#h2o.shutdown(prompt = FALSE)
options(java.parameters = "-Dai.h2o.disable.xgboost=true")
library(h2o)
h2o.init()
```

----------- RANDOM FOREST ---------

```{r}
# Convert training data to H2O frame
set.seed(123)
h2o.init(max_mem_size = "16G")
train_h2o <- as.h2o(training_data)

train_h2o[, "label"] <- as.factor(train_h2o[, "label"])

x <- colnames(training_data)[-which(names(training_data) == "label")]  # All columns except the target
y <- colnames(training_data[1])

# Train the H2O Random Forest Model
rf_model <- h2o.randomForest(
  x = x,                          # Predictors (column names)
  y = y,                          # Target (name of the target column)
  training_frame = train_h2o,     # Training data
  ntrees = 100,                   # Number of trees
  max_depth = 15,                 # Maximum tree depth
  mtries = 1000,                  # Features considered per split
  min_rows = 20,                  # Minimum rows per leaf
  sample_rate = 0.8               # Row sampling rate
)

# Retrieve variable importance
importance <- h2o.varimp(rf_model)
# Plot variable importance
h2o.varimp_plot(rf_model, num_of_features =75)
head(importance[order(importance$relative_importance, decreasing = TRUE), ], 75)

print(importance)

cumulative_importance <- 0
selected_features <- c()
for (i in 1:nrow(importance)) {
  cumulative_importance <- cumulative_importance + importance$percentage[i]
  selected_features <- c(selected_features, importance$variable[i])
  if (cumulative_importance >= 0.95) break
}
print(selected_features)

length(selected_features)
```
```{r}
rf_model_selected <- h2o.randomForest(
  x = selected_features,
  y = y,
  training_frame = train_h2o,
  ntrees = 100,
  max_depth = 15,
  mtries = length(selected_features),
  min_rows = 20,
  sample_rate = 0.8
)
```


```{r}
library(PRROC)

val_h2o <- as.h2o(validation_data[,-1])
predictions_rf <- (h2o.predict(rf_model_selected, val_h2o))
predictions_rf <- as.data.frame(predictions_rf)
predictions_rf <- as.numeric(predictions_rf$p1)  

# Initialize data frames to store results
threshold_results_rf <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric())

# Define a sequence of threshold values
threshold_values <- seq(0, 1, by = 0.001)

# Helper function to calculate balanced accuracy for a given set of predictions and thresholds
calculate_balanced_accuracy_1 <- function(predictions, y_actual, thresholds, model_name) {
  results <- data.frame(Threshold = numeric(), BalancedAccuracy = numeric(), Model = character())
  
  for (threshold in thresholds) {
    # Convert probabilities to binary predictions
    predicted_classes <- ifelse(predictions > threshold, 1, 0)
    
    # Create confusion matrix
    conf_matrix <- table(Predicted = predicted_classes, Actual = y_actual)
    
    # Extract TP, TN, FP, FN (handle missing categories with tryCatch)
    tn <- tryCatch(conf_matrix[1, 1], error = function(e) 0)
    tp <- tryCatch(conf_matrix[2, 2], error = function(e) 0)
    fp <- tryCatch(conf_matrix[2, 1], error = function(e) 0)
    fn <- tryCatch(conf_matrix[1, 2], error = function(e) 0)
    
    # Calculate balanced accuracy
    balanced_acc <- 0.5 * (tp / (tp + fn + 1e-6) + tn / (tn + fp + 1e-6)) # Add small value to avoid division by zero
    
    # Append results
    results <- rbind(results, data.frame(Threshold = threshold, BalancedAccuracy = balanced_acc, Model = model_name))
  }
  return(results)
}

length(val_h2o)
# Calculate balanced accuracy for Lasso and Elastic Net
threshold_results_rf<- calculate_balanced_accuracy_1(predictions_rf, y_val, threshold_values, "Random Forest")

# Combine results for plotting
threshold_results <- rbind(threshold_results_rf, threshold_results_EN, threshold_results_L)


# Plot Balanced Accuracy vs Threshold

ggplot(threshold_results, aes(x = Threshold, y = BalancedAccuracy, color= Model)) +
  geom_line(size = 1) +
  labs(
    title = "Balanced Accuracy vs Threshold",
    x = "Threshold",
    y = "Balanced Accuracy"
  ) +
  theme_minimal()

# Find and print maximum balanced accuracy for each model
max_balanced_accuracy_L <- threshold_results_L[which.max(threshold_results_L$BalancedAccuracy), ]
max_balanced_accuracy_EN <- threshold_results_EN[which.max(threshold_results_EN$BalancedAccuracy), ]

cat("Maximum Balanced Accuracy (Random Forest):\n")
cat("Threshold:", max_balanced_accuracy_L$Threshold, "\n")
cat("Balanced Accuracy:", max_balanced_accuracy_L$BalancedAccuracy, "\n")



```


```{r}
# Make predictions on the test data to mesure performance
test_h2o <- as.h2o(testing_data[,-1])
predictions_rf <- (h2o.predict(rf_model_selected, test_h2o))
predictions_rf <- as.data.frame(predictions_rf)
predictions_rf <- as.numeric(predictions_rf$p1)  

```


```{r}
predicted_classes_RF <- ifelse(predictions_rf > 0.034, 1, 0) #Using optimal threshold
```

```{r}
conf_matrix_RF <- table(Predicted =  predicted_classes_RF , Actual = y_true)
print("Confusion Matrix Random Forest:")
print(conf_matrix_RF)
```

```{r}
cat("The Balanced Accuracy of Lasso on the test set is:", balanced_accuracy_L, "\n")
cat("The Balanced Accuracy of Elastic Net on the test set is:", balanced_accuracy_EN, "\n")
balanced_accuracy_RF <- calculate_balanced_accuracy(conf_matrix_RF)
cat("The Balanced Accuracy of Random Forest on the test set is:", balanced_accuracy_RF, "\n")
```


```{r}
# Initialize an empty data frame
model_results <- data.frame(
  Model = character(),
  BalancedAccuracy = numeric(),
  NumFeatures = numeric()
)

# Add results dynamically
model_results <- rbind(model_results, data.frame(Model = "Lasso", BalancedAccuracy = balanced_accuracy_L, NumFeatures = length(selected_features_nonzero_1d)))
model_results <- rbind(model_results, data.frame(Model = "Elastic Net", BalancedAccuracy = balanced_accuracy_EN, NumFeatures = length(selected_features_nonzero_3d)))
model_results <- rbind(model_results, data.frame(Model = "Random Forest", BalancedAccuracy = balanced_accuracy_RF, NumFeatures = length(selected_features)))
# View the updated data frame
print(model_results)
```





