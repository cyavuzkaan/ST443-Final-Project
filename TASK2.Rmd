---
title: "TASK2"
output: html_document
date: "2024-11-13"
---

--- LIBRARIES ---

```{r}
library(data.table)
library(dplyr)
library(ggplot2)
library(caret)
library(ISLR)
```

```{r}
data <- read.csv("data2.csv.gz")
```

Binding: Indicates that the compound interacts or attaches effectively to the thrombin target site.(1)
Non-binding: Indicates that the compound does not interact effectively with thrombin.(0)
In the context of drug discovery, thrombin is a protein that could be a target for drugs aimed at affecting blood clotting, and identifying compounds that bind to thrombin can be a critical step in developing new medications.

```{r}
set.seed(123)
na_counts <- sapply(data, function(x) sum(is.na(x)))
totalna <- sum(na_counts)
print(totalna)

```

```{r}
# Compare Binding vs Non-binding
binding_data <- data[data[, 1] == 1, ]
count <- 0  # Initialize count
for (i in seq_len(nrow(data))) {
  if (data[i, 1] == -1) {
    count <- count + 1
  }
}

binding_count = nrow(data) - count
non_binding_count = count
print(non_binding_count)
ggplot(data, aes(x = data[, 1])) + geom_bar() + labs(x = "Binding Status", y = "Count")

```



```{r}
feature_sum <- colSums(data[, 2:100001])
hist(feature_sum, breaks = 50, main = "Distribution of Feature Sums", xlab = "Sum of binary feature values")

```

```{r}
head(data)
```

```{r}
# Iterate through label and make them 0 if they are -1, 1 otherwise
for (i in 1:nrow(data)) {
  if (data[i, 1] == -1) {
    data[i, 1] <- 0  
  } else {
    data[i, 1] <- 1
  }
}
print(data)
```


```{r}
# Find duplicate columns
unique_data <- data[, !duplicated(as.list(data))]
num_repeated <- ncol(data) - ncol(unique_data)
cat("The new dataset has dimensions:", dim(unique_data), "\n")
```

To compare our models we mesure their performance on unseen data. So we split the dataset in training and test
```{r}
# Use 80% of the data for training
end_training <- floor(nrow(unique_data) * 0.8)
# Split the data into training and testing sets
training_data <- unique_data[1:end_training, ] 
testing_data <- unique_data[(end_training + 1):nrow(unique_data), ]
print(dim(training_data))
print(dim(testing_data))

```


Lets build Lasso and Elastic Net with the training data
```{r}
library(Matrix)
library(glmnet)

#create dep. var
y_train <- training_data[,1]
y_true <- testing_data[,1]

#create training and testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")

features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")


#LASSO CLEAN
cv_fit_1d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1)
selected_features_1d <- coef(cv_fit_1d, s = "lambda.min")
selected_features_1d<- as.matrix(selected_features_1d)
selected_features_nonzero_1d <- selected_features_1d[selected_features_1d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_1d)," features according to LASSO in unique dataset","\n")
print(selected_features_nonzero_1d)
```


```{r}
#Elastic Net CLEAN
cv_fit_3d <- cv.glmnet(sparse_data_train, y_train, family = "binomial", parallel = TRUE, alpha= 1/2)
selected_features_3d <- coef(cv_fit_3d, s = "lambda.min")
selected_features_3d<- as.matrix(selected_features_3d)
selected_features_nonzero_3d <- selected_features_3d[selected_features_3d != 0, ]
cat("There are a total of ",length(selected_features_nonzero_3d)," features according to Elastic Net in unique dataset","\n")
print(selected_features_nonzero_3d)
```
Plot the choice of lamda
```{r}
## plot of the solution path, i.e. estimated coefficients vs log (lambda), where lambda is the tuning parameter
plot(cv_fit_1d, xvar="lambda", label= TRUE)
plot(cv_fit_3d, xvar="lambda", label= TRUE)
```
I commented the ridge regression out of the code because it keeps all the features and just makes the coefficeints smaller while the other models actually removes features by shrinking they coefficeints to 0.

Now we us the models on unseen data to predict the labels and evaluate the performance
Because we have unbalanced data instead of using the normal ROC and AUC we use the PR ROC and PR AUC
```{r} 

library(PRROC)
# Calculate PR AUC for predictions_1d (Lasso)
predictions_1d <- predict(cv_fit_1d, sparse_data_test, s = "lambda.min", type = "response") #unseen data prediction
pr_1d <- pr.curve(scores.class0 = predictions_1d, weights.class0 = y_test, curve = TRUE)
cat("PR AUC (Lasso):", pr_1d$auc.integral, "\n")

# Calculate PR AUC for predictions_3d (Elastic Net)
predictions_3d <- predict(cv_fit_3d, sparse_data_test, s = "lambda.min", type = "response")
pr_3d <- pr.curve(scores.class0 = predictions_3d, weights.class0 = y_test, curve = TRUE)
cat("PR AUC (Elastic Net):", pr_3d$auc.integral, "\n")


df_1d <- data.frame(Recall = pr_1d$curve[, 1], Precision = pr_1d$curve[, 2], Threshold = pr_1d$curve[, 3], Model = "Lasso")
df_3d <- data.frame(Recall = pr_3d$curve[, 1], Precision = pr_3d$curve[, 2], Threshold = pr_3d$curve[, 3], Model = "Elastic Net")
pr_data <- bind_rows(df_1d, df_3d)


# Compute F1 scores for each threshold to find the optimal
pr_data <- pr_data %>%
  mutate(F1 = 2 * (Precision * Recall) / (Precision + Recall + 1e-6))  # Add small value to avoid division by zero

# Find optimal threshold for each model (maximizing F1 score)
optimal_thresholds <- pr_data %>%
  group_by(Model) %>%
  filter(F1 == max(F1)) %>%
  summarize(Optimal_Threshold = Threshold[1], Max_F1 = F1[1])

# Print optimal thresholds
print(optimal_thresholds)

# Plot Precision-Recall Curves with ggplot2
ggplot(pr_data, aes(x = Recall, y = Precision, color = Model)) +
  geom_line(size = 1) +
  labs(
    title = "Precision-Recall Curves for Lasso and Elastic Net",
    x = "Recall",
    y = "Precision",
    color = "Model"
  ) +
  theme_minimal()
```


```{r}
# Measure performance on unseen data using the true y
# Convert probabilities to binary predictions using the optimal threshold for each model (tuning thew parameters)
predicted_classes_lasso <- ifelse(predictions_1d > 0.4048683, 1, 0)
accuracy <- mean(predicted_classes_lasso == y_true)
cat("The accuracy of Lasso on the test set is:", accuracy, "\n")

predicted_classes_EN <- ifelse(predictions_3d > 0.2443623, 1, 0) #
accuracy <- mean(predicted_classes_EN == y_true)
cat("The accuracy of Elastic Net on the test set is:", accuracy, "\n")

```
We tuned the parameters of each moddel such that it uses thye optimal threshold and we obatain the same accracy but 
we get abigger AUC 

We can test on how well the 2 model perform top choose the best: 
```{r}
# Cross validation errors when we training the models
lasso_errord <- min(cv_fit_1d$cvm)
mix_errord <- min(cv_fit_3d$cvm)

cat("Lasso CV Error:", lasso_errord, "\n")
cat("Elastic CV Net Error:", mix_errord, "\n")
```
```{r}
conf_matrix_lasso <- table(Predicted = predicted_classes_lasso, Actual = y_test)
print("Confusion Matrix Lasso:")
print(conf_matrix_lasso)

conf_matrix_EN <- table(Predicted =  predicted_classes_EN , Actual = y_test)
print("Confusion Matrix Elastic:")
print(conf_matrix_EN)
```


----------- RANDOM FOREST ---------

```{r}
library(randomForest)
library(ranger)

# Reduce feature size or sample size if needed
selected_features <- c("feature1", "feature2", "y_train")
training_subset <- training_data[sample(1:nrow(training_data), size = 0.5 * nrow(training_data)), selected_features]

# Train with ranger
rf_model <- ranger(y_train ~ ., data = training_subset, num.trees = 100, mtry = 5, max.depth = 20, importance = "impurity")

# Extract importance scores and sort them
feature_importance <- importance(rf_model)
sorted_importance <- sort(feature_importance[, 1], decreasing = TRUE)

```

XGBOOST 
FIRST STEP: Create train and test data.

```{r}
library(Matrix)
library(glmnet)
library(caTools)
set.seed(1)
features1a <- unique_data[,-1]
sparse_data1a <- as(features1a, "sparseMatrix")

data1_cleanedbin <- unique_data
data1_cleanedbin[, 1] <- ifelse(data1_cleanedbin[, 1] == -1, 0, 1)

splitboost <- sample.split(y,0.8)
trainboost <- data1_cleanedbin[splitboost,]
testboost <- data1_cleanedbin[!splitboost,]

```



```{r}
# Load the necessary library
library(xgboost)


# Separate the target ('label') and features for both trainboost and testboost
X_train <- as.matrix(trainboost[, -which(names(trainboost) == "label")])  # Exclude 'label' column for features
y_train <- as.numeric(trainboost$label)  # Extract the 'label' column as the target

X_test <- as.matrix(testboost[, -which(names(testboost) == "label")])  # Exclude 'label' column for features
y_test <- as.numeric(testboost$label)  # Extract the 'label' column as the target

# Convert to DMatrix format for XGBoost
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)
```



```{r}
library(xgboost)

# Simplified grid for essential parameters
param_grid <- expand.grid(
  max_depth = c(4, 6, 8), #tree depth
  eta = c(0.1, 0.3), #Learning rate F_t+1 = eta * T_t + F_t 
  subsample = c(0.6, 0.8), #fraction of rows
  colsample_bytree = c(0.8) #fraction of features
)

# Track results
best_model <- NULL
best_logloss <- Inf

for (i in 1:nrow(param_grid)) {
  params <- list(
    booster = "gbtree",
    objective = "binary:logistic",
    eval_metric = "logloss",
    max_depth = param_grid$max_depth[i],
    eta = param_grid$eta[i],
    subsample = param_grid$subsample[i],
    colsample_bytree = param_grid$colsample_bytree[i]
  )
  
  # Cross-validation for current parameters
  cv <- xgb.cv(
    params = params,
    data = dtrain,
    nrounds = 50,              # Keep this small to limit computation
    nfold = 5,                 # Stratified 5-fold CV
    early_stopping_rounds = 10,
    verbose = FALSE
  )
  
  # Update the best model if logloss improves
  min_logloss <- min(cv$evaluation_log$test_logloss_mean)
  if (min_logloss < best_logloss) {
    best_logloss <- min_logloss
    best_model <- params
  }
}
# STORE BEST_MODEL PARAMETERS

# Initialize a data frame to store results
if (!exists("model_results")) {
  model_results <- data.frame(
    eta = numeric(),
    max_depth = integer(),
    subsample = numeric(),
    colsample_bytree = numeric(),
    logloss = numeric(),
    stringsAsFactors = FALSE
  )
}

# Append the best_model parameters and log loss
model_results <- rbind(
  model_results,
  data.frame(
    eta = best_model$eta,
    max_depth = best_model$max_depth,
    subsample = best_model$subsample,
    colsample_bytree = best_model$colsample_bytree,
    logloss = best_logloss  # Use the corresponding logloss for the best model
  )
)

# Print the updated table
print(model_results)


print(best_model)



```

```{r}
# Assuming `best_model` is a list of the optimal parameters found during tuning
# If you have specific values for the best parameters, replace `best_model` fields directly.

final_params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eval_metric = "logloss",
  eta = best_model$eta,                 # Replace with the optimal eta value
  max_depth = best_model$max_depth,     # Replace with the optimal max_depth
  subsample = best_model$subsample,     # Replace with the optimal subsample value
  colsample_bytree = best_model$colsample_bytree # Replace with optimal colsample_bytree
)

# Train the model with the optimal parameters
final_model <- xgb.train(
  params = final_params, 
  data = dtrain, 
  nrounds = 100,                        # You can adjust the number of rounds based on early stopping results
  watchlist = list(train = dtrain, test = dtest), 
  early_stopping_rounds = 10,           # Stop training if no improvement is seen in 10 rounds
  verbose = 0                           # Optional: set to 0 for silent training
)

# Make predictions on the test data
final_predictions <- predict(final_model, dtest)

# Convert probabilities to binary predictions (0 or 1)
final_predictions_binary <- ifelse(final_predictions > 0.5, 1, 0)

# Evaluate the final model
final_accuracy <- sum(final_predictions_binary == y_test) / length(y_test)
print(paste("Final Accuracy:", final_accuracy))

# Confusion matrix
final_conf_matrix <- table(Predicted = final_predictions_binary, Actual = y_test)
print("Confusion Matrix:")
print(final_conf_matrix)


```

Stopping. Best iteration:
[18]	train-logloss:0.086565	test-logloss:0.209445

[1] "Final Accuracy: 0.933823529411765"
[1] "Confusion Matrix:"
         Actual
Predicted   0   1
        0 485  27
        1   9  23


```{r}
xgb.save(final_model, "xgboost_final_model.model")


importance_matrix <- xgb.importance(model = final_model)
print(importance_matrix)

top_features <- importance_matrix[1:30, ]  # Select the top 30 features

# Plot the top 30 features
xgb.plot.importance(importance_matrix = top_features, 
                    main = "Top 30 Most Important Features",
                    xlab = "Feature Importance")

# xgb.plot.importance(importance_matrix)
```


```{r}
# Load necessary libraries
library(ROCR)
library(ggplot2)

# Generate prediction and performance objects
pred <- prediction(final_predictions, y_test)  # Using predicted probabilities and true labels
perf <- performance(pred, "prec", "rec")       # Precision vs Recall (TPR)

# Extract precision and recall (TPR) values
precision <- perf@y.values[[1]]  # Precision values
recall <- perf@x.values[[1]]     # Recall (TPR) values

# Create a data frame for plotting
precision_recall_data <- data.frame(Recall = recall, Precision = precision)

# Plot Precision vs. TPR (Recall)
ggplot(precision_recall_data, aes(x = Recall, y = Precision)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title = "Precision vs TPR (Recall)",
    x = "True Positive Rate (Recall)",
    y = "Precision"
  ) +
  theme_minimal()

```

```{r}
# Load necessary libraries
library(pROC)

# Calculate ROC curve and AUC
roc_curve <- roc(y_test, final_predictions)  # Use true labels and predicted probabilities
auc_value <- auc(roc_curve)                 # Calculate the AUC value

# Print the AUC value
print(paste("AUC:", auc_value))

# Plot the ROC curve
plot(roc_curve, col = "blue", lwd = 2, main = "ROC Curve")
abline(a = 0, b = 1, col = "red", lty = 2)  # Add a diagonal line for random classifier
legend("bottomright", legend = paste("AUC =", round(auc_value, 3)), col = "blue", lwd = 2)

```

```{r}
# Load necessary libraries
library(Matrix)
library(glmnet)
library(pROC)
library(ROCR)
library(ggplot2)
library(xgboost)

# Data preparation
data1_cleaned <- unique_data
features1a <- data1_cleaned[, -1]
sparse_data1a <- as(features1a, "sparseMatrix")
y <- data[, 1]

# Lasso Model (Alpha = 1)
cv_fit_lasso <- cv.glmnet(sparse_data1a, y, family = "binomial", alpha = 1)
predictions_lasso <- predict(cv_fit_lasso, sparse_data1a, s = "lambda.min", type = "response")

# Elastic Net Model (Alpha = 0.5)
cv_fit_elastic_net <- cv.glmnet(sparse_data1a, y, family = "binomial", alpha = 0.5)
predictions_elastic_net <- predict(cv_fit_elastic_net, sparse_data1a, s = "lambda.min", type = "response")

predictions_boosting <- final_predictions

# ROC Curves and AUCs
roc_lasso <- roc(y, predictions_lasso, quiet = TRUE)
roc_elastic_net <- roc(y, predictions_elastic_net, quiet = TRUE)
roc_boosting <- roc(y, final_predictions, quiet = TRUE)

auc_lasso <- auc(roc_lasso)
auc_elastic_net <- auc(roc_elastic_net)
auc_boosting <- auc(roc_boosting)

# Plot ROC Curves
plot(roc_lasso, col = "blue", lwd = 2, main = "ROC Curves for Lasso, Elastic Net, and Boosting")
lines(roc_elastic_net, col = "red", lwd = 2)
lines(roc_boosting, col = "green", lwd = 2)
legend("bottomright", legend = c(
  paste("Lasso (AUC =", round(auc_lasso, 3), ")"),
  paste("Elastic Net (AUC =", round(auc_elastic_net, 3), ")"),
  paste("Boosting (AUC =", round(auc_boosting, 3), ")")
), col = c("blue", "red", "green"), lwd = 2)

# Precision vs TPR (Recall)
# Prepare predictions and calculate precision/recall for all models
models <- list(
  Lasso = predictions_lasso,
  ElasticNet = predictions_elastic_net,
  Boosting = predictions_boosting
)

precision_recall_data <- data.frame()

for (model_name in names(models)) {
  pred <- prediction(models[[model_name]], y)
  perf <- performance(pred, "prec", "rec")
  
  precision <- perf@y.values[[1]]
  recall <- perf@x.values[[1]]
  
  precision_recall_data <- rbind(precision_recall_data, data.frame(
    Recall = recall,
    Precision = precision,
    Model = model_name
  ))
}

# Plot Precision vs TPR
ggplot(precision_recall_data, aes(x = Recall, y = Precision, color = Model)) +
  geom_line(size = 1) +
  labs(title = "Precision vs TPR for Lasso, Elastic Net, and Boosting",
       x = "True Positive Rate (Recall)",
       y = "Precision") +
  theme_minimal()


```


