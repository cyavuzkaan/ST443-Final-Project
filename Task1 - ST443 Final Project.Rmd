---
title: "Task 1 - ST443 Project"
author: "Finbar Rhodes"
date: "`r Sys.Date()`"
output: html_document
---

```{r libraries, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(443)
library(ggplot2)
library(tidyverse)
library(MASS)
library(class)
library(pROC)
library(caret) 
#library(adabag) # For the AdaBoost algorithm
library(mboost)   # For gamboost
library (gbm) # Gradient Boosting Machines
library(xgboost) #Extreme Gradient Boosting
library(ranger)
library(e1071)
library(lattice)
library(PRROC)

```

```{r reading in data, echo=TRUE}
task1data <- read.csv("/Users/finbarrhodes/Documents/ST443/Final Project/data1.csv.gz")
task1data$label <- if_else(task1data$label == "TREG", 1, 0)
```

Here we adjust the *label* column to a factor column, with *TREG* being 1 and *CD4+T* being 0

```{r label-to-factor}
table(task1data$label, useNA = "ifany")
```

## T1.1

```{r missing data, echo=TRUE}

nas <- task1data |> is.na() |> colSums() |> table()
if (nas[1] == ncol(task1data)){
  print("We have that there are no missing data in any columns which will be hekpful in our analysis")
}
```

```{r overall sparsity, echo=FALSE}
total_zeros <- sum(rowSums(task1data == 0))
total_entries <- 5471 * 4124
(total_zeros / total_entries) |> round(digits = 3)
```

Our given data is reasonably sparse. In fact, below we have shown that approximately 66.2% of this dataset are zero entries. We can explore sparsity across the covariates as well.

```{r sparsity across features, echo=TRUE}
covariate_sparsities <- data.frame(Gene = colnames(task1data)[-1], 
                                   Sparsity = rep(0, ncol(task1data)-1)) 

for (i in 2:ncol(task1data)){
  count <- length(which(task1data[,i] == 0))
  covariate_sparsities$Sparsity[i-1] <- count / nrow(task1data)
}

summary(covariate_sparsities)
ggplot(covariate_sparsities, aes(Sparsity)) + 
  geom_histogram(color = "black", fill = "white" ,bins = 40) + 
  ggtitle("Distribution of Sparsity Rates Across Covariates") + 
  theme_bw()


barplot(colMeans(task1data[2:ncol(task1data)]))

```

can ask the question if all features are truly of use

```{r heatmap}

task1matrix <- sapply(task1data, as.numeric) |> as.matrix()

pal <- colorRampPalette(c("red", "yellow"), space = "rgb") 
levelplot(task1matrix, main="Task 1 Data Heatmap", xlab=" ", ylab=" ", col.regions=pal(40), cuts=3, at=seq(0,1,0.5)) #, useRaster = TRUE)


```

## T1.2

Below we will shuffle the rows in our dataset, then split them into *training*, *validation*, and *test* sets.

```{r data setup, echo=TRUE}
task1data <- task1data[sample(1:nrow(task1data)), ]

training_index <- floor(nrow(task1data) * 0.7)  # 70% for training
validation_index <- floor(nrow(task1data) * 0.85)  # Next 15% for validation, leaving 15% for test set

# Split the data into training, validation, and testing sets
task1_train <- task1data[1:training_index, ]  # 70% of the data
task1_validation <- task1data[(training_index + 1):validation_index, ]  # 15% of the data
task1_test <- task1data[(validation_index + 1):nrow(task1data), ]  # Remaining 15% of the data

# train <- sample(c(TRUE, FALSE), nrow(task1data), replace=TRUE, prob=c(0.75,0.25))
# test <- !train
# task1_train <- task1data[train,]
# task1_test <- task1data[test,]
```

### Base Models

#### LDA

```{r lda fit}
lda_fit <- lda(label ~ ., data = task1_train)
```

```{r lda predict}
lda_pred_full <-  predict(lda_fit, task1_test)
lda_pred <- predict(lda_fit, task1_test)$class

test_labels <- task1_test$label
lda_conf_matrix <- table(lda_pred, test_labels)
```

```{r lda eval, echo=TRUE}
lda_conf_matrix

eval_metrics(str = "LDA", conf = lda_conf_matrix)

```

#### Logistic Regression

```{r logistic regression, echo=TRUE}
logistic_fit <- glm(label ~ ., data = task1_train, family = binomial)
logistic_probs <-  predict(logistic_fit, newdata = task1_test, type = "response")
```

```{r logit eval}
logistic_pred <-  rep(0, nrow(task1_test))
logistic_pred[logistic_probs > .5] <-  1

logistic_conf_matrix <- table(logistic_pred, task1_test$label)
eval_metrics(str = "Logisitc Regression", conf = logistic_conf_matrix)
```

#### QDA

In running this classifier, there is a problem inherent in our data: there are too few observations in the two groups in the training set for qda() to run. In our training set, there are 2540 CD4+T's (0's) and 1624 TREG's (1's). In order for qda() to run properly, we can only have, at a maximum, 1624 covariates or columns in the dataset. At this juncture, we can consider dimenstion reduction methods.

```{r qda, echo=TRUE}
#dummy_data <- task1data[,1:1624]
#qda_fit <- MASS::qda(label ~ ., data = dummy_data, subset = train)

```

#### KNN

```{r knn, echo=TRUE}
knn <- knn3(label ~., data = task1_train, k=1)
knn_preds <- predict(knn, newdata = task1_test, type = "prob")
knn_conf_matrix <- table(knn_preds[,2], task1_test$label)
eval_metrics("KNN", knn_conf_matrix)
```

#### GBDT

```{r GBDT}

gbdt <- gbm(label ~ ., data = task1_train, 
                   distribution = "gaussian", 
                   n.trees = 1000, 
                   interaction.depth = 4, 
                   shrinkage = .001)

gbdt_probs <- predict(gbdt, newdata = task1_test, n.trees = 1000, type = "response")

gbdt_preds <-  rep(0, nrow(task1_test))
gbdt_preds <- ifelse(gbdt_probs > .5, 1, 0)

gbdt_conf_matrix <- table(gbdt_preds, task1_test$label)

```

#### Random Forest

```{r echo=TRUE}

rf <- ranger(label~., 
             data = task1_train, 
             mtry = ncol(task1_train) |> sqrt() |> round(digits = 0),
             importance = "none",
             write.forest = TRUE,
             num.trees = 1000,
             classification = TRUE,
             verbose = TRUE)

rf_preds <- predict(rf, data=task1_test)$predictions

rf_conf_matrix <- table(rf_preds, task1_test$label)

```

#### SVM

```{r}
cost = 10 #This is the regularisation parameter.
svmfit = svm(label ~ ., data = task1_train, kernel = "linear", cost = cost, scale = FALSE)
svm_predictions <- predict(svmfit, task1_test, type = "response")
summary(svm_predictions)

```

## T1.2.PCA

```{r pca fit & data}
pca <- prcomp(~., data = task1data[2:ncol(task1data)])
top10_weights <- pca$rotation[,1:10]
reduced_data <- pca$x[,1:10] |> as.data.frame()
reduced_data$label <- task1data$label
```

```{r pca plot, echo=TRUE}
table_pca <- rbind(pca$rotation[,1:20], summary(pca)$importance[,1:20])


# var_props <- data.frame(PC = 1:10, 'PVE' = table_pca['Proportion of Variance',])
"
ggplot(var_props, aes(x=PC, y=PVE)) + 
  geom_line() + 
  geom_point() + 
  xlab('Principal Components') + 
  ylab('Proportion of Variance Remaining') + 
  xlim(0, .5)
  "



par(mfrow=c(1,1))
plot(table_pca['Proportion of Variance',], 
     type = 'l', 
     lwd = 5, 
     col = 'blue', 
     xlim = c(1,20), 
     ylim = c(0,.05),
     main = 'Proportion of Variance Explained by Principal Components', 
     xlab = 'Principal Components', 
     ylab = 'Proportion of Variance Unexplained', 
     axes = TRUE)

```

Same dataset splitting as earlier; PCA maintains row order by matrix multiplication.

```{r data setup, echo=TRUE}
reduced_train <- reduced_data[1:training_index, ] 
reduced_validation <- reduced_data[(training_index + 1):validation_index, ]
reduced_test <- reduced_data[(validation_index + 1):nrow(task1data), ] 
```

### Models with PCA

#### LDA

```{r pca lda fit}
pca_lda_fit <- lda(label ~ ., data = reduced_train)
```

```{r pca lda predict}
pca_lda_pred_full <-  predict(pca_lda_fit, reduced_test)
pca_lda_pred <- predict(pca_lda_fit, reduced_test)$class

pca_test_labels <- reduced_test$label
pca_lda_conf_matrix <- table(pca_lda_pred, pca_test_labels)
```

```{r pca lda eval, echo=TRUE}
pca_lda_conf_matrix
eval_metrics(str = "LDA", conf = pca_lda_conf_matrix)

```

#### Logistic Regression

```{r pca logistic regression, echo=TRUE}
pca_logistic_fit <- glm(label ~ ., data = reduced_train, family = binomial)
pca_logistic_probs <-  predict(pca_logistic_fit, newdata = reduced_test, type = "response")
```

```{r pca logit eval}
pca_logistic_pred <-  rep(0, nrow(reduced_test))
pca_logistic_pred[pca_logistic_probs > .5] <-  1

# tail(cbind(task1_train$label,logistic_pred))
pca_logistic_conf_matrix <- table(pca_logistic_pred, reduced_test$label)
eval_metrics(str = "Logisitc Regression", conf = pca_logistic_conf_matrix)
```

#### QDA

In running this classifier, there is a problem inherent in our data: there are too few observations in the two groups in the training set for qda() to run. In our training set, there are 2540 CD4+T's (0's) and 1624 TREG's (1's). In order for qda() to run properly, we can only have, at a maximum, 1624 covariates or columns in the dataset. At this juncture, we can consider dimenstion reduction methods.

```{r qda, echo=TRUE}
pca_qda_fit <- MASS::qda(label ~ ., data = reduced_train)

pca_qda_pred_full <-  predict(pca_qda_fit, reduced_test)
pca_qda_pred <- predict(pca_qda_fit, reduced_test)$class

pca_test_labels <- reduced_test$label
pca_qda_conf_matrix <- table(pca_qda_pred, pca_test_labels)

```

#### KNN

```{r pca knn, echo=TRUE}

pca_knn <- knn3(label ~., data = reduced_train, k=1)
pca_knn_preds <- predict(pca_knn, newdata = reduced_test, type = "prob")
pca_knn_conf_matrix <- table(pca_knn_preds[,2], task1_test$label)
#eval_metrics("KNN", pca_knn_conf_matrix)

ppca_knn <- knn3Train(reduced_train[2:ncol(reduced_train)], 
                 reduced_test[2:ncol(reduced_test)], 
                 reduced_train$label, 
                 k = 1, 
                 use.all = TRUE) 
# this returns the assigned classes from test cases

ppca_knn_conf_matrix <- table(ppca_knn, task1_test$label)

rbind(eval_metrics("knn3Train", ppca_knn_conf_matrix), eval_metrics("knn3", pca_knn_conf_matrix))
```

#### GBDT

```{r pca GBDT}

pca_gbdt <- gbm(label ~ ., data = reduced_train, 
                   distribution = "gaussian", 
                   n.trees = 1000, 
                   interaction.depth = 4, 
                   shrinkage = .001, 
                   cv.folds = 5, 
                   verbose = TRUE)


pca_gbdt_probs <- predict(pca_gbdt, newdata = reduced_test, n.trees = 1000, type = "response")

pca_gbdt_preds <-  rep(0, nrow(task1_test))
pca_gbdt_preds <- ifelse(pca_gbdt_probs > .5, 1, 0)

pca_gbdt_conf_matrix <- table(pca_gbdt_preds, reduced_test$label)

```

#### Random Forest

```{r pca rf, echo=TRUE}

pca_rf <- ranger(label~., 
             data = reduced_train, 
             mtry = ncol(reduced_train) |> sqrt() |> round(digits = 0),
             importance = "none",
             write.forest = TRUE,
             num.trees = 1000,
             classification = TRUE,
             verbose = TRUE)

pca_rf_preds <- predict(pca_rf, data=reduced_test)$predictions

pca_rf_conf_matrix <- table(pca_rf_preds, reduced_test$label)

eval_metrics("RF", pca_rf_conf_matrix)

```

#### SVM

```{r pca svm}
cost = 10 #This is the regularisation parameter.
pca_svmfit = svm(label ~ ., data = reduced_train, kernel = "linear", cost = cost, scale = FALSE)
pca_svm_predictions <- predict(pca_svmfit, reduced_test)
```

## T1.3

Classifiers to tweak:
- KNN 
- GBDT 
- Random Forest


In this section, we bring in the validation set (15% of the data) to tune hyperparameters, settle on an optimal setup, and then reevaluate on the test set. 


#### KNN

In the *k*-Nearest Neighbor classifier, here we are tweaking the main parameter *k*, the number of neighbors taken into account in fitting the model.

```{r knn k-tuning, echo=TRUE}
knn_validation_preds <- predict(knn, newdata = task1_validation, type = "prob")
knn_validation_conf_matrix <- table(knn_validation_preds[,2], task1_test$label)
knn_metrics <- eval_metrics(str = "1 NN", conf = knn_validation_conf_matrix)

pca_knn_validation_preds <- predict(pca_knn, newdata = reduced_validation, type = "prob")
pca_knn_validation_conf_matrix <- table(pca_knn_validation_preds[,2], reduced_test$label)
pca_knn_metrics <- eval_metrics(str = "1 NN", conf = pca_knn_validation_conf_matrix)

for (k in 2:20){
  model <- knn3Train(task1_train[2:ncol(task1_train)], 
                     task1_validation[2:ncol(task1_validation)], 
                     task1_train$label, 
                     k=k, 
                     prob = TRUE, 
                     use.all=TRUE)
  
  pca_model <- knn3Train(reduced_train[2:ncol(reduced_train)], 
                     reduced_validation[2:ncol(reduced_validation)], 
                     reduced_train$label, 
                     k=k, 
                     prob = TRUE, 
                     use.all=TRUE)  

  conf_matrix <- table(model, task1_validation$label)
  pca_conf_matrix <- table(pca_model, reduced_validation$label)
  
  
  knn_metrics[k,] <- eval_metrics(paste(as.character(k), "NN"), conf_matrix)
  pca_knn_metrics[k,] <- eval_metrics(paste(as.character(k), "NN"), pca_conf_matrix)
  
  print(k)
}

knn_metrics
pca_knn_metrics

ggplot(knn_metrics, aes(x=(1:nrow(knn_metrics)), F1)) + 
  geom_line() +
  geom_point() +
  geom_vline(xintercept = which.max(knn_metrics$F1), linetype = "dashed", color = "red") + 
  xlim(1,nrow(knn_metrics)) + 
  xlab("Number of Neighbors") + 
  ylab("F1 Score") +
  ggtitle("Selecting k to Maximize F1 Score") +
  theme_linedraw() + 
  theme(plot.title = element_text(hjust = 0.5)) 

ggplot(pca_knn_metrics, aes(x=(1:nrow(pca_knn_metrics)), F1)) + 
  geom_line() +
  geom_point() +
  geom_vline(xintercept = which.max(pca_knn_metrics$F1), linetype = "dashed", color = "blue") + 
  xlim(1,nrow(pca_knn_metrics)) + 
  xlab("Number of Neighbors") + 
  ylab("F1 Score") +
  ggtitle("Selecting k to Maximize F1 Score for Dimension-Reduced Models") +
  theme_linedraw() + 
  theme(plot.title = element_text(hjust = 0.5)) 

```


```{r final knn, echo=TRUE}
# KNN Model Predictions

tuned_1nn <- knn3Train(task1_train[2:ncol(task1_train)], 
                       task1_test[2:ncol(task1_test)], 
                       task1_train$label, 
                       k=5, 
                       prob = TRUE, 
                       use.all=TRUE)

tuned_1nn_conf_matrix <- table(tuned_1nn, task1_test$label)

pca_tuned_knn <- knn3Train(reduced_train[2:ncol(reduced_train)], 
                       reduced_test[2:ncol(reduced_test)], 
                       reduced_train$label, 
                       k=9, 
                       prob = TRUE, 
                       use.all=TRUE)

pca_tuned_knn_conf_matrix <- table(pca_tuned_knn, reduced_test$label)


# knn3 method
#ttuned_knn <- knn3(label ~., data = task1_train, k=5)

#ttuned_knn_probs <- predict(ttuned_knn, newdata = task1_validation, type = "prob")

#ttuned_knn_preds <-  rep(0, nrow(task1_test))
#ttuned_knn_preds <- ifelse(ttuned_knn_probs[,2] > .5, 1, 0)

#ttuned_knn_conf_matrix <- table(ttuned_knn_preds, task1_test$label)

```

#### Random Forest
```{r rf threshold selection}

# Predict on the Validation Set

# Random Forest Model Predictions
rf_validation_preds <- predict(rf, data = task1_validation[2:ncol(task1_validation)], type = "response")$predictions

# Calculate PR Curves Using Validation Labels (y_val)

# PR Curve for Lasso    Precision-Recall
rf_validation_pr_curve <- pr.curve(scores.class0 = rf_validation_preds, weights.class0 = task1_validation$label, curve = TRUE)

cat("PR AUC (Random Forest, Validation Set):", rf_validation_pr_curve$auc.integral, "\n") # output


# Create Data Frames for Precision-Recall Curves
rf_curve_df <- data.frame(Model = "Random Forest", 
                          Recall = rf_validation_pr_curve$curve[, 1], 
                          Precision = rf_validation_pr_curve$curve[, 2], 
                          Threshold = rf_validation_pr_curve$curve[, 3])

```




#### GBDT


One classifier to improve upon is Gradient Boosted Decision Trees, namely by tuning the hyperparameter lambda that can be thought of as a shrinkage parameter.


```{r, GBTM tuning from workshops, echo=TRUE}
# Define grid of lambda (shrinkage) values to evaluate
lambda_grid <- c(0.001, 0.01,0.03,0.05)

# Initialize a vector to store test errors for each lambda
test_errors <- numeric(length(lambda_grid))

# Loop over each lambda value
for (i in seq_along(lambda_grid)) {
  lambda <- lambda_grid[i]

  # Train the gbm model with the current lambda (shrinkage) value
  gbm_model <- gbm(label ~ ., data = task1_train, 
                   distribution = "gaussian", 
                   n.trees = 1000, 
                   interaction.depth = 4, 
                   shrinkage = lambda, 
                   cv.folds = 5, 
                   verbose = FALSE)
  
  # Make predictions on the test set using the optimal number of trees
  predictions <- predict(gbm_model, newdata = task1_test, n.trees = 1000)
  
  
  
  # Calculate the Mean Squared Error on the test set
  test_errors[i] <- mean((predictions - task1_test$label)^2)
}

# code is from a workshop 

# Combine lambda values and test errors into a data frame for plotting
error_df <- data.frame(lambda = lambda_grid, test_error = test_errors)

# Plot the test error over the range of lambda values
ggplot(error_df, aes(x = lambda, y = test_error)) + 
  geom_line() +
  geom_point() +
  labs(title = "Test Error vs Lambda (Shrinkage)", 
       x = "Lambda (Shrinkage)", 
       y = "Test Mean Squared Error") +
  theme_minimal()
```


```{r gbdt threshold selection}

# Predict on the Validation Set

# Random Forest Model Predictions
gbdt_validation_preds <- predict(gbdt, newdata = task1_validation[2:ncol(task1_validation)], n.trees = 1000, type = "response") # $predictions

# Calculate PR Curves Using Validation Labels (y_val)

# PR Curve for Lasso    Precision-Recall
gbdt_validation_pr_curve <- pr.curve(scores.class0 = gbdt_validation_preds, weights.class0 = task1_validation$label, curve = TRUE)

cat("PR AUC (Random Forest, Validation Set):", gbdt_validation_pr_curve$auc.integral, "\n") # output


# Create Data Frames for Precision-Recall Curves
gbdt_curve_df <- data.frame(Model = "Gradient Boosted Decision Trees", 
                          Recall = gbdt_validation_pr_curve$curve[, 1], 
                          Precision = gbdt_validation_pr_curve$curve[, 2], 
                          Threshold = gbdt_validation_pr_curve$curve[, 3])

```


```{r gbdt optimal threshold implementation}

# Compute F1 Scores and Find Optimal Thresholds
gbdt_curve_df <- gbdt_curve_df |> mutate(F1 = 2 * (Precision * Recall) / (Precision + Recall + 1e-6))  # Avoid division by zero

gbdt_threshold <- gbdt_curve_df[which.max(gbdt_curve_df$F1),4]

tuned_gbdt_probs <- predict(gbdt, newdata = task1_test, n.trees = 1000, type = "response")

tuned_gbdt_preds <-  rep(0, nrow(task1_test))
tuned_gbdt_preds <- ifelse(tuned_gbdt_probs > gbdt_threshold, 1, 0)

tuned_gbdt_conf_matrix <- table(tuned_gbdt_preds, task1_test$label)

```



## Model Evaluation

```{r eval_metrics function, echo=TRUE}

eval_metrics <- function(str, conf){
  # setup
  TN <- conf[[1]]
  FN <- conf[[2]]
  FP <- conf[[3]]
  TP <- conf[[4]]
  
  # metrics
  accuracy <- 1 - ((FN + FP) / sum(conf))
  BA <- .5 * (TP / (TP + FN)) + .5 * (TN / (TN + FP))
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  F1 <- 2 * (precision * recall) / (precision + recall + 1e-6)
  
  # output
  return(data.frame("Method" = str,
                    "Accuracy" = accuracy |> round(digits = 3),
                    "Balanced Accuracy" = BA |> round(digits = 3),
                    "F1" = F1 |> round(digits = 3)))
}

```

```{r summary table}

# need to add ROC / AUC info

base_summary_table <- rbind(eval_metrics("LDA", lda_conf_matrix), 
                       c("QDA", rep(NA,3)),
                       eval_metrics("Logistic Regression", logistic_conf_matrix),
                       eval_metrics("1NN", knn_conf_matrix),
                       eval_metrics("Random Forest", rf_conf_matrix),
                       eval_metrics("GBDT", gbdt_conf_matrix),
                       c("SVM", rep(NA,3)))

pca_summary_table <- rbind(eval_metrics("LDA", pca_lda_conf_matrix), 
                       eval_metrics("QDA", pca_qda_conf_matrix),
                       eval_metrics("Logistic Regression", pca_logistic_conf_matrix),
                       eval_metrics("KNN", pca_knn_conf_matrix),
                       eval_metrics("Random Forest", pca_rf_conf_matrix),
                       eval_metrics("GBDT", pca_gbdt_conf_matrix),
                       c("SVM", rep(NA,3)))


tuned_summary_table <- rbind(eval_metrics("5NN", tuned_1nn_conf_matrix), 
                       eval_metrics("GBDT with Optimal Threshold", tuned_gbdt_conf_matrix),
                       c("Random Forest", rep(NA,3)))

# tuned_summary_table 


#summary_table$AUC <- c(auc(roc(task1_test$label, lda_pred_full$posterior[,2], levels = c(0,1), direction = "<")),
 #                      auc(roc(task1_test$label, lda_pred_full$posterior[,2], levels = c(0,1), direction = "<")),
  #                     auc(roc(task1_test$label, lda_pred_full$posterior[,2], levels = c(0,1), direction = "<")),
   #                    auc(roc(task1_test$label, lda_pred_full$posterior[,2], levels = c(0,1), direction = "<")))
  
#lda_roc <- roc(task1_test$label, lda_pred_full$posterior[,2], levels = c(0, 1), direction = "<")
#auc(lda_roc)

rbind(eval_metrics("1NN (Base Case)", knn_conf_matrix),
      eval_metrics("5NN", tuned_1nn_conf_matrix),
      eval_metrics("PCA 1NN", pca_knn_conf_matrix),
      eval_metrics("PCA 9NN", pca_tuned_knn_conf_matrix))

rbind(eval_metrics("Base RF", rf_conf_matrix),
      eval_metrics("PCA RF", pca_rf_conf_matrix))
```
