---
title: "Task 1 - ST443 Project"
author: "Finbar Rhodes"
date: "`r Sys.Date()`"
output: html_document
---

```{r libraries, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(443)
library(ggplot2)
library(tidyverse)
library(MASS)
library(class)
library(pROC)
library(caret) 
#library(adabag) # For the AdaBoost algorithm
library(mboost)   # For gamboost
library (gbm) # Gradient Boosting Machines
library(xgboost) #Extreme Gradient Boosting
library(randomForest)
library(ranger)
library(e1071)
library(lattice)
```

```{r reading in data, echo=TRUE}
task1data <- read.csv("/Users/finbarrhodes/Documents/ST443/Final Project/data1.csv.gz")
task1data$label <- if_else(task1data$label == "TREG", 1, 0)
```

Here we adjust the *label* column to a factor column, with *TREG* being 1 and *CD4+T* being 0
```{r label-to-factor}
table(task1data$label, useNA = "ifany")
```


## T1.1

```{r missing data, echo=TRUE}

abc <- task1data |> is.na() |> colSums() |> table()
if (abc[1] == ncol(task1data)){
  print("We have that there are no missing data in any columns which will be hekpful in our analysis")
}
```

```{r sparsity, echo=FALSE}
total_zeros <- sum(rowSums(task1data == 0))
total_entries <- 5471 * 4124
total_zeros / total_entries
```
Our given data is reasonably sparse. In fact, below we have shown that approximately 66.2% of this dataset are zero entries. We can explore sparsity across the covariates as well. 

``` {r covariate sparsity, echo=TRUE}
covariate_sparsities <- data.frame(Gene = colnames(task1data)[-1], 
                                   Sparsity = rep(0, ncol(task1data)-1)) 

for (i in 2:ncol(task1data)){
  count <- length(which(task1data[,i] == 0))
  covariate_sparsities$Sparsity[i-1] <- count / nrow(task1data)
}

summary(covariate_sparsities)
ggplot(covariate_sparsities, aes(Sparsity)) + 
  geom_histogram(color = "black", fill = "white" ,bins = 40) + 
  ggtitle("Distribution of Sparsity Rates Across Covariates") + 
  theme_bw()


barplot(colMeans(task1data[2:ncol(task1data)]))

```
can ask the question if all features are truly of use

```{r heatmap}

task1matrix <- sapply(task1data, as.numeric) |> as.matrix()

pal <- colorRampPalette(c("red", "yellow"), space = "rgb") 
levelplot(task1matrix, main="Task 1 Data Heatmap", xlab=" ", ylab=" ", col.regions=pal(40), cuts=3, at=seq(0,1,0.5))#, useRaster = TRUE)


```


## T1.2 
```{r eval_metrics, echo=TRUE}

eval_metrics <- function(str, conf){
  # setup
  TN <- conf[[1]]
  FN <- conf[[2]]
  FP <- conf[[3]]
  TP <- conf[[4]]
  
  # metrics
  accuracy <- 1 - ((FN + FP) / sum(conf))
  BA <- .5 * (TP / (TP + FN)) + .5 * (TN / (TN + FP))
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  F1 <- (precision * recall) / (precision + recall)
  
  # output
  return(data.frame("Method" = str,
                    "Accuracy" = accuracy,
                    "Balanced Accuracy" = BA,
                    "F1" = F1))
}

methods <- c(c("Random Forest", rf_conf_matrix), 
             c("KNN", knn_conf_matrix), 
             c("Logistic Regression", logistic_conf_matrix), 
             c("LDA", lda_conf_matrix))

```

```{r data setup, echo=TRUE}
train <- sample(c(TRUE, FALSE), nrow(task1data), replace=TRUE, prob=c(0.75,0.25))
test <- !train
task1_train <- task1data[train,]
task1_test <- task1data[test,]
```
### Models
#### LDA

```{r lda fit}
lda_fit <- lda(label ~ ., data = task1data, subset = train)
```

```{r lda predict}
lda_pred_full <-  predict(lda_fit, task1_test)
lda_pred <- predict(lda_fit, task1_test)$class

test_labels <- task1data$label[test]
lda_conf_matrix <- table(lda_pred, test_labels)
```



```{r lda eval, echo=TRUE}
lda_conf_matrix

eval_metrics(str = "LDA", conf = lda_conf_matrix)

```


#### Logistic Regression
```{r logistic regression, echo=TRUE}
logistic_fit <- glm(label ~ ., data = task1data, subset = train, family = binomial)
logistic_probs <-  predict(logistic_fit, newdata = task1_test, type = "response")
```

```{r logit eval}
logistic_pred <-  rep(0, nrow(task1_test))
logistic_pred[logistic_probs > .5] <-  1

# tail(cbind(task1_train$label,logistic_pred))
logistic_conf_matrix <- table(logistic_pred, task1_test$label)
eval_metrics(str = "Logisitc Regression", conf = logistic_conf_matrix)
```

#### QDA
In running this classifier, there is a problem inherent in our data: there are too few observations in the two groups in the training set for qda() to run. In our training set, there are 2540 CD4+T's (0's) and 1624 TREG's (1's). In order for qda() to run properly, we can only have, at a maximum, 1624 covariates or columns in the dataset. At this juncture, we can consider dimenstion reduction methods. 
```{r qda, echo=TRUE}
dummy_data <- task1data[,1:1624]
qda_fit <- MASS::qda(label ~ ., data = dummy_data, subset = train)

```

#### KNN
```{r knn, echo=TRUE}

knn <- knn(task1_train, task1_test, task1data$label[train], k=1)

knn_conf_matrix <- table(knn, task1data$label[test])

eval_metrics(knn_conf_matrix)
```

#### GBDT

```{r GBDT}

gbdt <- gbm(label ~ ., data = task1_train, 
                   distribution = "gaussian", 
                   n.trees = 1000, 
                   interaction.depth = 4, 
                   shrinkage = .001, 
                   cv.folds = 5, 
                   verbose = TRUE)

gbdt_preds <- predict(gbdt, newdata = task1_test, n.trees = 1000)

```


#### Random Forest

```{r echo=TRUE}

rf <- ranger(label~., 
             data = task1_train, 
             mtry = ncol(task1_train) |> sqrt() |> round(digits = 0),
             importance = "none",
             write.forest = TRUE,
             num.trees = 1000,
             classification = TRUE,
             verbose = TRUE)

rf_preds <- predict(rf, data=task1_test)$predictions

rf_conf_matrix <- table(rf_preds, task1data$label[test])

```


#### SVM
```{r}
cost = 10 #This is the regularisation parameter.
svmfit = svm(label ~ ., data = task1_train, kernel = "linear", cost = cost, scale = FALSE)
svm_predictions <- predict(svmfit, task1_test)
```

### T1.2.PCA 
```{r pca fit & data}
pca <- prcomp(~., data = task1data[2:ncol(task1data)])

top10_weights <- pca$rotation[,1:10]
reduced_data <- pca$x[,1:10] |> as.data.frame()
reduced_data$label <- task1data$label

```


```{r pca plot, echo=TRUE}
summary(pca$x)[1:10]
View(pca$x)


table_pca <- rbind(pca$rotation, summary(pca)$importance)
print(table_pca[ ,1:10])

par(mfrow=c(1,1))
plot(table_pca['Proportion of Variance',], type = 'l', lwd = 5, col = 'blue', xlim = c(1,4), main = 'PC proportions of total variance', xlab = 'PC', ylab = 'Proportion of variance', axes = FALSE)
axis(1, 1:10)
axis(2)
```

```{r data setup, echo=TRUE}
reduced_train <- reduced_data[train,]
reduced_test <- reduced_data[test,]
```
### Models with PCA

#### LDA
```{r pca lda fit}
pca_lda_fit <- lda(label ~ ., data = reduced_data, subset = train)
```

```{r pca lda predict}
pca_lda_pred_full <-  predict(pca_lda_fit, reduced_test)
pca_lda_pred <- predict(pca_lda_fit, reduced_test)$class

pca_test_labels <- reduced_data$label[test]
pca_lda_conf_matrix <- table(pca_lda_pred, pca_test_labels)
```



```{r pca lda eval, echo=TRUE}
pca_lda_conf_matrix
eval_metrics(str = "LDA", conf = pca_lda_conf_matrix)

```


#### Logistic Regression
```{r pca logistic regression, echo=TRUE}
pca_logistic_fit <- glm(label ~ ., data = reduced_data, subset = train, family = binomial)
pca_logistic_probs <-  predict(pca_logistic_fit, newdata = reduced_test, type = "response")
```

```{r pca logit eval}
pca_logistic_pred <-  rep(0, nrow(reduced_test))
pca_logistic_pred[pca_logistic_probs > .5] <-  1

# tail(cbind(task1_train$label,logistic_pred))
pca_logistic_conf_matrix <- table(pca_logistic_pred, reduced_test$label)
eval_metrics(str = "Logisitc Regression", conf = pca_logistic_conf_matrix)
```

#### QDA
In running this classifier, there is a problem inherent in our data: there are too few observations in the two groups in the training set for qda() to run. In our training set, there are 2540 CD4+T's (0's) and 1624 TREG's (1's). In order for qda() to run properly, we can only have, at a maximum, 1624 covariates or columns in the dataset. At this juncture, we can consider dimenstion reduction methods. 
```{r qda, echo=TRUE}
dummy_data <- reduced_data[,1:1624]
pca_qda_fit <- MASS::qda(label ~ ., data = dummy_data, subset = train)

```

#### KNN
```{r pca knn, echo=TRUE}

pca_knn <- knn(reduced_train, reduced_test, reduced_data$label[train], k=1)

pca_knn_conf_matrix <- table(pca_knn, reduced_data$label[test])

eval_metrics("KNN", pca_knn_conf_matrix)
```

#### GBDT

```{r pca GBDT}

pca_gbdt <- gbm(label ~ ., data = reduced_train, 
                   distribution = "gaussian", 
                   n.trees = 1000, 
                   interaction.depth = 4, 
                   shrinkage = .001, 
                   cv.folds = 5, 
                   verbose = TRUE)

pca_gbdt_preds <- predict(pca_gbdt, newdata = reduced_test, n.trees = 1000)

```


#### Random Forest

```{r pca rf, echo=TRUE}

pca_rf <- ranger(label~., 
             data = reduced_train, 
             mtry = ncol(reduced_train) |> sqrt() |> round(digits = 0),
             importance = "none",
             write.forest = TRUE,
             num.trees = 1000,
             classification = TRUE,
             verbose = TRUE)

pca_rf_preds <- predict(pca_rf, data=reduced_test)$predictions

pca_rf_conf_matrix <- table(pca_rf_preds, reduced_data$label[test])

eval_metrics("RF", pca_rf_conf_matrix)

```


#### SVM
```{r pca svm}
cost = 10 #This is the regularisation parameter.
pca_svmfit = svm(label ~ ., data = reduced_train, kernel = "linear", cost = cost, scale = FALSE)
pca_svm_predictions <- predict(pca_svmfit, reduced_test)
```


## T1.3

Classifiers to tweak:
- KNN
- GBTM
- Random Forest

#### KNN

In the *k*-Nearest Neighbor classifier, here we are tweaking the main parameter *k*, the number of neighbors taken into account in fitting the model.  
```{r knn v2, echo=TRUE}
knn_metrics <- eval_metrics(str = "1NN", conf = knn_conf_matrix)
for (k in 1:10){
  fit <- knn(task1_train, task1_test, task1data$label[train], k=k)
  
  conf_matrix <- table(fit, task1data$label[test])
  
  rbind(knn_metrics, eval_metrics(cat(k,"NN"), conf_matrix))
}

knn_metrics

```

#### GBDT

One classifier to improve upon is Gradient Boosted Decision Trees, namely by tuning the hyperparameter lambda that can be thought of as a shrinkage parameter.
```{r, GBTM v2, echo=TRUE}
# Define grid of lambda (shrinkage) values to evaluate
lambda_grid <- c(0.001, 0.01,0.03,0.05)

# Initialize a vector to store test errors for each lambda
test_errors <- numeric(length(lambda_grid))
```

```{r}
# Loop over each lambda value
for (i in seq_along(lambda_grid)) {
  lambda <- lambda_grid[i]

  # Train the gbm model with the current lambda (shrinkage) value
  gbm_model <- gbm(label ~ ., data = task1_train, 
                   distribution = "gaussian", 
                   n.trees = 1000, 
                   interaction.depth = 4, 
                   shrinkage = lambda, 
                   cv.folds = 5, 
                   verbose = FALSE)
  
  # Make predictions on the test set using the optimal number of trees
  predictions <- predict(gbm_model, newdata = task1_test, n.trees = 1000)
  
  
  
  # Calculate the Mean Squared Error on the test set
  test_errors[i] <- mean((predictions - task1_test$label)^2)
}
```

```{r}
# Combine lambda values and test errors into a data frame for plotting
error_df <- data.frame(lambda = lambda_grid, test_error = test_errors)

# Plot the test error over the range of lambda values
ggplot(error_df, aes(x = lambda, y = test_error)) +
  geom_line() +
  geom_point() +
  labs(title = "Test Error vs Lambda (Shrinkage)", 
       x = "Lambda (Shrinkage)", 
       y = "Test Mean Squared Error") +
  theme_minimal()
```


## Model Evaluation
```{r summary table}


# need to add ROC / AUC info

base_summary_table <- rbind(eval_metrics("LDA", lda_conf_matrix), 
                       c("QDA", rep(NA,3)),
                       eval_metrics("Logistic Regression", logistic_conf_matrix),
                       eval_metrics("KNN", knn_conf_matrix),
                       eval_metrics("Random Forest", rf_conf_matrix),
                       c("GBDT", rep(NA,3)),
                       c("SVM", rep(NA,3)))

pca_summary_table <- rbind(eval_metrics("LDA", pca_lda_conf_matrix), 
                       c("QDA", rep(NA,3)),
                       eval_metrics("Logistic Regression", pca_logistic_conf_matrix),
                       eval_metrics("KNN", pca_knn_conf_matrix),
                       eval_metrics("Random Forest", pca_rf_conf_matrix),
                       c("GBDT", rep(NA,3)),
                       c("SVM", rep(NA,3)))


# tuned_summary_table 


#summary_table$AUC <- c(auc(roc(task1_test$label, lda_pred_full$posterior[,2], levels = c(0,1), direction = "<")),
 #                      auc(roc(task1_test$label, lda_pred_full$posterior[,2], levels = c(0,1), direction = "<")),
  #                     auc(roc(task1_test$label, lda_pred_full$posterior[,2], levels = c(0,1), direction = "<")),
   #                    auc(roc(task1_test$label, lda_pred_full$posterior[,2], levels = c(0,1), direction = "<")))
  
#lda_roc <- roc(task1_test$label, lda_pred_full$posterior[,2], levels = c(0, 1), direction = "<")
#auc(lda_roc)


```





