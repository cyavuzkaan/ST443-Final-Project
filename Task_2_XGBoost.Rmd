---
title: "Untitled"
output: html_document
date: "2024-12-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r}
library(data.table)
library(dplyr)
library(ggplot2)
library(caret)
library(ISLR)
# STEP 1 DATA COLLECTION ---------

data <- read.csv("data2.csv.gz")
# Iterate through label and make them 0 if they are -1, 1 otherwise

# STEP 1.0. DATA PREPARATION ---------
for (i in 1:nrow(data)) {
  if (data[i, 1] == -1) {
    data[i, 1] <- 0  
  } else {
    data[i, 1] <- 1
  }
}
# Find duplicate columns
unique_data <- data[, !duplicated(as.list(data))]
num_repeated <- ncol(data) - ncol(unique_data)
cat("The new dataset has dimensions:", dim(unique_data), "\n")
# Calculate row indices for each split
set.seed(12)  # Set seed for reproducibility
unique_data <- unique_data[sample(nrow(unique_data)), ]
n <- nrow(unique_data)
end_training <- floor(n * 0.7)  # 70% for training
end_validation <- floor(n * 0.85)  # Next 10% for validation

# Split the data into training, validation, and testing sets
training_data <- unique_data[1:end_training, ]  # 70% of the data
validation_data <- unique_data[(end_training + 1):end_validation, ]  # 10% of the data
testing_data <- unique_data[(end_validation + 1):n, ]  # Remaining 20% of the data


# DATA SPLITING ---------
#create dep. var
y_train <- training_data[,1]
y_valid <- validation_data[,1]
y_test <- testing_data[,1]
#create training, validation and testing matrix with ind.var
features_train <- training_data[,-1]
sparse_data_train <- as(features_train, "sparseMatrix")
features_val <- validation_data[,-1]
sparse_data_valid <- as(features_val, "sparseMatrix")
features_test<- testing_data[,-1]
sparse_data_test <- as(features_test, "sparseMatrix")

```

```{r}
library(xgboost)
library(Matrix)
library(glmnet)
library(caTools)
library(SparseM)

# DATA CONVERTION ---------
# Convert to DMatrix format for XGBoost
dtrain_xg <- xgb.DMatrix(data = sparse_data_train, label = y_train)
dvalid_xg <- xgb.DMatrix(data = sparse_data_valid, label = y_valid)
dtest_xg <- xgb.DMatrix(data = sparse_data_test, label = y_test)



```

```{r}
library(xgboost)
library(data.table)

# Define the parameter grid
param_grid_xg <- expand.grid(
  max_depth = c(4, 6, 8),             # Tree depth
  eta = c(0.01, 0.05, 0.1, 0.2),      # Learning rate
  subsample = c(0.6, 0.8),            # Fraction of rows sampled per boosting round
  colsample_bytree = c(0.6, 0.8),     # Fraction of features sampled per tree
  min_child_weight = c(1, 3, 5),      # Minimum sum of instance weight needed in a child
  gamma = c(0, 0.1, 0.5),             # Minimum loss reduction for further partitioning
  alpha = c(1, 2, 3, 5)                 # L2 regularization term
)

# Track the best model and results
best_model_xg <- NULL
best_logloss_xg <- Inf
best_params_xg <- NULL

# Create a data frame to store results
cv_results <- data.frame(
  max_depth = integer(),
  eta = numeric(),
  subsample = numeric(),
  colsample_bytree = numeric(),
  min_child_weight = numeric(),
  gamma = numeric(),
  lambda = numeric(),
  logloss = numeric(),
  stringsAsFactors = FALSE
)

# Cross-validation loop
set.seed(123)  # For reproducibility
for (i in 1:nrow(param_grid_xg)) {
  
  params_xg <- list(
    booster = "gbtree",
    objective = "binary:logistic",
    eval_metric = "logloss",
    max_depth = param_grid_xg$max_depth[i],
    eta = param_grid_xg$eta[i],
    subsample = param_grid_xg$subsample[i],
    colsample_bytree = param_grid_xg$colsample_bytree[i],
    min_child_weight = param_grid_xg$min_child_weight[i],
    gamma = param_grid_xg$gamma[i],
    alpha = param_grid_xg$alpha[i]
  )
  
  # Perform cross-validation
  cv <- xgb.cv(
    params = params_xg,
    data = dtrain_xg,
    nrounds = 100,
    nfold = 5,                        # 5-fold cross-validation
    early_stopping_rounds = 10,       # Stop if no improvement for 10 rounds
    verbose = FALSE
  )
  
  # Get the best logloss for this combination of parameters
  min_logloss <- min(cv$evaluation_log$test_logloss_mean)
  
  # Store the results
  cv_results <- rbind(cv_results, data.frame(
    max_depth = param_grid_xg$max_depth[i],
    eta = param_grid_xg$eta[i],
    subsample = param_grid_xg$subsample[i],
    colsample_bytree = param_grid_xg$colsample_bytree[i],
    min_child_weight = param_grid_xg$min_child_weight[i],
    gamma = param_grid_xg$gamma[i],
    alpha = param_grid_xg$alpha[i],
    logloss = min_logloss
  ))
  
  # Update the best model if the logloss improves
  if (min_logloss < best_logloss_xg) {
    best_logloss_xg <- min_logloss
    best_model_xg <- params_xg
    best_params_xg_detailed <- params_xg
  }
  print(i)
}

# Print the best parameters and the corresponding logloss
print("Best Hyperparameters for XGBoost:")
print(best_params_xg)
print(paste("Best Logloss:", best_logloss_xg))

# View the complete cross-validation results
print(cv_results)

```



```{r}
set.seed(12)
# HYPERPARAMETER TUNNING WITH CV using TRAIN data ---------

# Simplified grid for essential parameters
param_grid_xg <- expand.grid(
  max_depth = c(4, 6, 8), # tree depth
  eta = c(0.1 , 0.2 , 0.15), # Learning rate F_t+1 = eta * T_t + F_t 
  subsample = c(0.6, 0.8), # fraction of rows
  colsample_bytree = c(0.6 , 0.8) # fraction of features
)

# Track results
best_model_xg <- NULL
best_logloss_xg <- Inf

for (i in 1:nrow(param_grid_xg)) {
  params_xg <- list(
    booster = "gbtree",
    objective = "binary:logistic",
    eval_metric = "logloss",
    max_depth = param_grid_xg$max_depth[i],
    eta = param_grid_xg$eta[i],
    subsample = param_grid_xg$subsample[i],
    colsample_bytree = param_grid_xg$colsample_bytree[i]
  )
  
  # Cross-validation for current parameters
  cv_xg <- xgb.cv(
    params = params_xg,
    data = dtrain_xg,
    nrounds = 50,              # Keep this small to limit computation
    nfold = 5,                 # Stratified 5-fold CV
    early_stopping_rounds = 10,
    verbose = FALSE
  )
  
  # Update the best model if logloss improves
  min_logloss_xg <- min(cv_xg$evaluation_log$test_logloss_mean)
  if (min_logloss_xg < best_logloss_xg) {
    best_logloss_xg <- min_logloss_xg
    best_model_xg <- params_xg
  }
}
# STORE BEST_MODEL PARAMETERS

# Initialize a data frame to store results
if (!exists("model_results_xg")) {
  model_results_xg <- data.frame(
    eta = numeric(),
    max_depth = integer(),
    subsample = numeric(),
    colsample_bytree = numeric(),
    logloss = numeric(),
    stringsAsFactors = FALSE
  )
}

# Append the best_model parameters and log loss
model_results_xg <- rbind(
  model_results_xg,
  data.frame(
    eta = best_model_xg$eta,
    max_depth = best_model_xg$max_depth,
    subsample = best_model_xg$subsample,
    colsample_bytree = best_model_xg$colsample_bytree,
    logloss = best_logloss_xg  # Use the corresponding logloss for the best model
  )
)

# Print the updated table
print(model_results_xg)
print(best_model_xg)

```


```{r}
set.seed(12)
initial_threshold <- sum(y_valid)/length(y_valid)
# Custom balanced accuracy function
balanced_accuracy <- function(preds, dtrain) {
  # Get the true labels
  labels <- getinfo(dtrain, "label")
  
  # Convert probabilities to binary predictions using a threshold of 0.5
  preds_binary <- ifelse(preds > initial_threshold, 1, 0)
  
  # Create confusion matrix components
  tn <- sum(preds_binary == 0 & labels == 0)
  tp <- sum(preds_binary == 1 & labels == 1)
  fp <- sum(preds_binary == 1 & labels == 0)
  fn <- sum(preds_binary == 0 & labels == 1)
  
  # Calculate balanced accuracy
  balanced_acc <- 0.5 * (tp / (tp + fn + 1e-6) + tn / (tn + fp + 1e-6))
  
  # Return a list with the metric name and its value
  return(list(metric = "balanced_accuracy", value = balanced_acc))
}


final_params_xg <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = best_model_xg$eta,                 # Replace with the optimal eta value
  max_depth = best_model_xg$max_depth,     # Replace with the optimal max_depth
  subsample = best_model_xg$subsample,     # Replace with the optimal subsample value
  colsample_bytree = best_model_xg$colsample_bytree # Replace with optimal colsample_bytree
)



# XG MODEL using TRAIN data -----------
final_model_xg <- xgb.train(
  params = final_params_xg, 
  data = dtrain_xg, 
  nrounds = 100,                        # You can adjust the number of rounds based on early stopping results
  watchlist = list(train = dtrain_xg), 
  early_stopping_rounds = 10,           # Stop training if no improvement is seen in 10 rounds
  feval = balanced_accuracy,
  maximize = TRUE,
  verbose = 0                           # Optional: set to 0 for silent training
)

# Make predictions on the test data
predictions_xg_train <- predict(final_model_xg, dtrain_xg) 
# Default THRESHOLD
predictions_binary_xg_train <- ifelse(predictions_xg_train > 0.5, 1, 0)

# Evaluate the final model
accuracy_xg_train <- sum(predictions_binary_xg_train == y_train) / length(y_train)
print(paste("Final Accuracy for Train data with 0.5:", accuracy_xg_train))


conf_matrix_xg_train <- table(Predicted = predictions_binary_xg_train, Actual = y_train) # Confusion matrix train
print("Confusion Matrix:")
print(conf_matrix_xg_train)


importance_matrix_xg_train <- xgb.importance(model = final_model_xg)

# Calculate cumulative gain for the features
importance_matrix_xg_train$cumulative_gain <- cumsum(importance_matrix_xg_train$Gain)

# Print the importance matrix with cumulative gain
print(importance_matrix_xg_train)

# Select the top 25 features
top_features_xg <- importance_matrix_xg_train[1:10, ]

# Plot the top 25 features
xgb.plot.importance(
  importance_matrix = top_features_xg, 
  main = "Top 25 Most Important Features",
  xlab = "Feature Importance"
)
```



```{r}
library(ggplot2)
set.seed(12)
# Initialize an empty data frame to store results
predictions_xg_valid <- predict(final_model_xg, dvalid_xg) 
threshold_results_xg_valid <- data.frame(Threshold_xg = numeric(), BalancedAccuracy_xg = numeric())

# Define a sequence of threshold values
threshold_values_xg_valid <- seq(0, 1, by = 0.001)

# Loop through each threshold value
for (threshold_xg in threshold_values_xg_valid) {
  # Convert probabilities to binary predictions based on the current threshold
  predicted_classes_xg_valid <- ifelse(predictions_xg_valid > threshold_xg, 1, 0)
  
  # Calculate the confusion matrix
  conf_matrix_xg_valid <- table(Predicted_xg = predicted_classes_xg_valid, Actual_xg = y_valid)
  
  # Extract TP, TN, FP, FN (use tryCatch for error handling in case of missing categories)
  tn_xg <- tryCatch(conf_matrix_xg_valid[1, 1], error = function(e) 0)
  tp_xg <- tryCatch(conf_matrix_xg_valid[2, 2], error = function(e) 0)
  fp_xg <- tryCatch(conf_matrix_xg_valid[2, 1], error = function(e) 0)
  fn_xg <- tryCatch(conf_matrix_xg_valid[1, 2], error = function(e) 0)
  
  # Calculate balanced accuracy
  balan_acc_xg_valid <- 0.5 * (tp_xg / (tp_xg + fn_xg + 1e-6) + tn_xg / (tn_xg + fp_xg + 1e-6)) # Add small value to avoid division by zero
  
  # Append the results to the data frame
  threshold_results_xg_valid <- rbind(threshold_results_xg_valid, data.frame(Threshold_xg = threshold_xg, BalancedAccuracy_xg = balan_acc_xg_valid))
}

# Print the results
# print(threshold_results_xg)

ggplot(threshold_results_xg_valid, aes(x = Threshold_xg, y = BalancedAccuracy_xg)) +
  geom_line(color = "black", size = 0.8) +
  geom_point(color = "grey", size = 1) +
  labs(
    title = "Balanced Accuracy vs. Threshold (XGBoost)",
    x = "Threshold",
    y = "Balanced Accuracy"
  ) +
  theme_minimal(base_size = 10) +
  theme(
    panel.grid.major = element_line(color = "grey90"),
    panel.grid.minor = element_blank(),
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.text = element_text(color = "black"),
    axis.title = element_text(face = "bold")
  )

max_balanced_accuracy_xg <- threshold_results_xg_valid[which.max(threshold_results_xg_valid$BalancedAccuracy_xg), ]

# Output the results
cat("Threshold with Maximum Balanced Accuracy (XGBoost):\n")
cat("Threshold_xg:", max_balanced_accuracy_xg$Threshold_xg, "\n")
cat("Balanced Accuracy_xg:", max_balanced_accuracy_xg$BalancedAccuracy_xg, "\n")

```



```{r}
set.seed(12)
# BEST THRESHOLD BY VALIDATING
# 
predictions_xg_test <- predict(final_model_xg, dtest_xg) 
predicted_classes_xg_test <- ifelse(predictions_xg_test > max_balanced_accuracy_xg$Threshold_xg , 1, 0)
# TO TEST A PARTICULAR THRESHOLD:
# predicted_classes_xg_test <- ifelse(predictions_xg_test > 0.0395 , 1, 0)

conf_matrix_xg_test <- table(Predicted = predicted_classes_xg_test, Actual = y_test)
print("Confusion Matrix Test XG:")
print(conf_matrix_xg_test)

tn_xg_t = conf_matrix_xg_test[1,1]
tp_xg_t = conf_matrix_xg_test[2,2]
fp_xg_t = conf_matrix_xg_test[2,1]
fn_xg_t = conf_matrix_xg_test[1,2]

balan_acc_xg_test = 0.5*( tp_xg_t / (tp_xg_t+fn_xg_t) + tn_xg_t / (tn_xg_t + fp_xg_t) )
print(balan_acc_xg_test)

```

--------------------------------------------------
 NOW IM GOING TO WORK WITH LESS FEATURES
--------------------------------------------------


```{r}
library(xgboost)
set.seed(12)
#accuracies <- data.frame()
# Initialize an empty data frame to store model results
model_accuracies <- data.frame(
  model_name = character(),
  max_depth = integer(),
  test_balanced_accuracy = numeric(),
  num_features = integer(),
  stringsAsFactors = FALSE
)

depth <- 2
final_params_xg_loop <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = best_model_xg$eta,                 # Replace with the optimal eta value
  max_depth = depth,     # Replace with the optimal max_depth
  subsample = best_model_xg$subsample,     # Replace with the optimal subsample value
  colsample_bytree = best_model_xg$colsample_bytree # Replace with optimal colsample_bytree
)



# XG MODEL using TRAIN data -----------
final_model_xg_loop <- xgb.train(
  params = final_params_xg_loop, 
  data = dtrain_xg, 
  nrounds = 100,                        # You can adjust the number of rounds based on early stopping results
  watchlist = list(train = dtrain_xg), 
  early_stopping_rounds = 10,           # Stop training if no improvement is seen in 10 rounds
  feval = balanced_accuracy,
  maximize = TRUE,
  verbose = 0                           # Optional: set to 0 for silent training
)

##

importance_matrix_xg_train <- xgb.importance(model = final_model_xg_loop)
num_features <- length(importance_matrix_xg_train$Feature)
  
  # ---------------------------
  # Make predictions on the test data
predictions_xg_test_loop <- predict(final_model_xg_loop, dtest_xg)

  # Use a fixed threshold (e.g., 0.0395) for classification
predicted_classes_xg_test_loop <- ifelse(predictions_xg_test_loop > max_balanced_accuracy_xg$Threshold_xg, 1, 0)

  # Generate the confusion matrix
conf_matrix_xg_test_loop <- table(Predicted = predicted_classes_xg_test_loop, Actual = y_test)

  # Extract TP, TN, FP, FN with error handling for missing classes
tn_xg_t_loop <- conf_matrix_xg_test_loop[1,1]
tp_xg_t_loop <- conf_matrix_xg_test_loop[2,2]
fp_xg_t_loop <- conf_matrix_xg_test_loop[2,1]
fn_xg_t_loop <- conf_matrix_xg_test_loop[1,2]

  # Calculate balanced accuracy
balan_acc_xg_test_loop <- 0.5 * (tp_xg_t_loop / (tp_xg_t_loop + fn_xg_t_loop + 1e-6) + tn_xg_t_loop / (tn_xg_t_loop + fp_xg_t_loop + 1e-6))
  
accuracies <- rbind(accuracies, data.frame(
  max_depth = depth,
  test_balanced_accuracy = round(balan_acc_xg_test_loop, 4),
   num_features = num_features
))

# Print the summary of accuracies for all models
print("Summary of Model Performance:")

print(accuracies)

```





